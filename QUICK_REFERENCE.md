# Quick Reference Guide

## ğŸš€ Getting Started (5 minutes)

```bash
# 1. Install
pip install -r requirements.txt

# 2. In one terminal: Start Ollama
ollama serve

# 3. In another terminal: Pull model
ollama pull nomic-embed-text

# 4. Back to first terminal: Run app
streamlit run app.py
```

Then open http://localhost:8501 and click "Index Documents" â†’ "Search"

---

## ğŸ“š Core Concepts at a Glance

### Embeddings
- Text â†’ 768-dimensional vectors
- Similar text = vectors close together
- Enables semantic similarity
- Generated by `src/embeddings.py` using Ollama

### Chunking
- Break documents into pieces
- Default: 500 characters per chunk
- Overlap: 100 characters (improves recall)
- Implemented in `src/chunking.py`

### Similarity
- Measure distance between vectors
- Cosine similarity: 0 (unrelated) to 1 (identical)
- `src/similarity.py` handles calculation

### Vector Database
- Store embeddings efficiently
- ChromaDB in `src/vector_store.py`
- Saves to `./data/chroma_db/`
- HNSW indexing for fast search

---

## ğŸ“ Project Files

```
src/
â”œâ”€â”€ config.py           # Configuration, settings
â”œâ”€â”€ ingestion.py        # Load PDF/TXT/MD files
â”œâ”€â”€ chunking.py         # Split text into chunks
â”œâ”€â”€ embeddings.py       # Generate embeddings (Ollama)
â”œâ”€â”€ similarity.py       # Calculate similarity
â”œâ”€â”€ vector_store.py     # ChromaDB integration
â””â”€â”€ search_engine.py    # Main orchestrator

data/
â”œâ”€â”€ documents/          # Your documents go here
â”‚   â”œâ”€â”€ machine_learning_intro.md
â”‚   â”œâ”€â”€ embeddings_guide.md
â”‚   â””â”€â”€ vector_databases.md
â””â”€â”€ chroma_db/          # Auto-created index

app.py                  # Streamlit web interface
README.md               # Full documentation
LEARNING_GUIDE.md       # Detailed educational content
EXAMPLE_QUERIES.md      # Example queries to try
```

---

## âš™ï¸ Configuration (.env)

```env
OLLAMA_MODEL=nomic-embed-text              # Embedding model
OLLAMA_BASE_URL=http://localhost:11434     # Ollama server
CHROMA_DB_PATH=./data/chroma_db            # Storage location
TOP_K=5                                    # Number of results
CHUNK_SIZE=500                             # Characters per chunk
CHUNK_OVERLAP=100                          # Overlap between chunks
```

### Configuration Impact

| Setting | Smaller | Larger | Impact |
|---------|---------|--------|--------|
| CHUNK_SIZE | More chunks, more specific | Fewer chunks, more context | Result granularity |
| CHUNK_OVERLAP | Less storage | More storage, better recall | Coverage completeness |
| TOP_K | Faster | Slower | Number of results |

---

## ğŸ”„ Data Flow

### Indexing
```
PDF/TXT/MD â†’ Ingest â†’ Chunk â†’ Embed â†’ Store in ChromaDB â†’ Disk
```

### Searching
```
Query â†’ Embed â†’ Search ChromaDB â†’ Rank by Similarity â†’ Return Results
```

---

## ğŸ§ª Example Python Code

### Basic Search

```python
from src.search_engine import SemanticSearchEngine

# Initialize
engine = SemanticSearchEngine()

# Index documents
engine.index_documents("./data/documents")

# Search
results = engine.search("What are embeddings?", top_k=3)

# Display
for result in results:
    print(f"Score: {result['similarity_score']}")
    print(f"Source: {result['source_document']}")
    print(f"Text: {result['text'][:100]}...")
```

### Direct Embedding

```python
from src.embeddings import EmbeddingFactory

# Create embedder
embedder = EmbeddingFactory.create("ollama")

# Embed text
vec = embedder.embed("What is machine learning?")
# Returns: [0.12, -0.45, 0.78, ..., -0.23]  (768 numbers)

# Embed multiple texts
texts = ["Hello", "Hi", "Goodbye"]
vecs = embedder.embed_batch(texts)
```

### Calculate Similarity

```python
from src.similarity import SimilarityMetricFactory
import numpy as np

# Create similarity metric
cosine = SimilarityMetricFactory.create("cosine")

# Compare two vectors
vec1 = [1, 0, 0]
vec2 = [0.9, 0.1, 0]

similarity = cosine.similarity(vec1, vec2)
print(similarity)  # ~0.98 (very similar)
```

### Chunking

```python
from src.chunking import ChunkerFactory

# Create chunker
chunker = ChunkerFactory.create("fixed", chunk_size=500, overlap=100)

# Split document
text = "Machine learning is... [long text]"
chunks = chunker.chunk(text, source_doc="ml_intro.md", page_number=1)

print(f"Created {len(chunks)} chunks")
for chunk in chunks:
    print(f"  ID: {chunk.chunk_id}")
    print(f"  Text: {chunk.text[:50]}...")
```

---

## ğŸ› Common Issues & Quick Fixes

| Issue | Solution |
|-------|----------|
| "Can't connect to Ollama" | Run `ollama serve` in another terminal |
| "Model not found" | Run `ollama pull nomic-embed-text` |
| "No documents found" | Add files to `./data/documents/` |
| "Search returns nothing" | Run index first in Streamlit UI |
| "Slow searches" | Reduce CHUNK_SIZE, reduce TOP_K |

---

## ğŸ“Š Similarity Score Guide

```
1.0  = Identical
0.8+ = Excellent match
0.7+ = Very good match
0.6+ = Good match
0.5+ = Fair match
< 0.5 = Weak match
```

---

## ğŸ¯ Key Takeaways

1. **Embeddings** convert text to vectors
2. **Chunking** breaks documents for better retrieval
3. **Similarity** measures vector closeness (cosine)
4. **Vector DB** stores and searches efficiently
5. **Trade-offs** between size, speed, and quality

---

## ğŸš€ Next Steps

1. Run the app: `streamlit run app.py`
2. Index documents (Streamlit UI)
3. Try example queries from EXAMPLE_QUERIES.md
4. Modify settings and observe impacts
5. Read LEARNING_GUIDE.md for deep dive
6. Experiment with code examples above
7. Extend with your own documents

---

## ğŸ“š Important Files to Read

- **README.md** - Project overview and troubleshooting
- **LEARNING_GUIDE.md** - Detailed concept explanations
- **EXAMPLE_QUERIES.md** - Sample queries and expected results
- **src/search_engine.py** - Main orchestrator (well documented)

---

## ğŸ’¡ Pro Tips

1. **Start simple:** Use example documents first
2. **Experiment:** Try different chunk sizes
3. **Understand trade-offs:** More overlap = better coverage but more storage
4. **Read code:** Docstrings explain concepts clearly
5. **Watch Ollama:** Monitor the Ollama terminal to see embeddings being generated

---

## ğŸ“ Getting Help

1. Check README.md troubleshooting section
2. Read docstrings in source files
3. Review LEARNING_GUIDE.md for concepts
4. Try example queries in EXAMPLE_QUERIES.md
5. Examine configuration impact in .env

Good luck learning semantic search! ğŸ“âœ¨
