{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7784542",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Dependencies\n",
        "\n",
        "Install and verify all required packages. We'll use Ollama for embeddings and LLM, ChromaDB for storage, and additional libraries for advanced analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "481bf504",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing packages...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All packages installed!\n",
            "\n",
            "üìù Notes:\n",
            "- Make sure Ollama is running: ollama serve\n",
            "- Pull models if needed:\n",
            "    ollama pull nomic-embed-text (recommended)\n",
            "    ollama pull mistral (for LLM answer generation)\n",
            "    ollama pull neural-search (alternative embedding model)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/bin/python -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = [\n",
        "    'chromadb>=0.5.0',\n",
        "    'ollama',\n",
        "    'numpy',\n",
        "    'matplotlib',\n",
        "    'scikit-learn',\n",
        "    'pandas',\n",
        "    'rank-bm25',\n",
        "    'requests',\n",
        "]\n",
        "\n",
        "print(\"Installing packages...\")\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")\n",
        "print(\"\\nüìù Notes:\")\n",
        "print(\"- Make sure Ollama is running: ollama serve\")\n",
        "print(\"- Pull models if needed:\")\n",
        "print(\"    ollama pull nomic-embed-text (recommended)\")\n",
        "print(\"    ollama pull mistral (for LLM answer generation)\")\n",
        "print(\"    ollama pull neural-search (alternative embedding model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9a54fb60",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ankit.jha/Documents/codes/llm_playground/RAG_101/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n",
            "\n",
            "üìä Ready for advanced semantic search experiments\n"
          ]
        }
      ],
      "source": [
        "# Core imports and setup\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# For text processing\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# For embeddings\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(\"\\nüìä Ready for advanced semantic search experiments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8df7f96d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Connected to Ollama at http://localhost:11434\n",
            "   Using model: nomic-embed-text\n",
            "\n",
            "‚úÖ Embedder initialized!\n"
          ]
        }
      ],
      "source": [
        "# Ollama embedder with timing support\n",
        "class TimedOllamaEmbedder:\n",
        "    \"\"\"Generate embeddings using Ollama with performance tracking.\"\"\"\n",
        "    \n",
        "    def __init__(self, model: str = \"nomic-embed-text\", base_url: str = \"http://localhost:11434\"):\n",
        "        self.model = model\n",
        "        self.base_url = base_url.rstrip(\"/\")\n",
        "        self.endpoint = f\"{self.base_url}/api/embed\"\n",
        "        self.timings = []\n",
        "        \n",
        "        # Check connection\n",
        "        try:\n",
        "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=2)\n",
        "            print(f\"‚úÖ Connected to Ollama at {self.base_url}\")\n",
        "            print(f\"   Using model: {self.model}\")\n",
        "        except:\n",
        "            print(f\"‚ùå Cannot connect to Ollama at {self.base_url}\")\n",
        "            print(f\"   Start with: ollama serve\")\n",
        "            raise\n",
        "    \n",
        "    def embed(self, text: str) -> Tuple[List[float], float]:\n",
        "        \"\"\"Generate embedding for a single text, returns (embedding, time_ms).\"\"\"\n",
        "        if not text.strip():\n",
        "            return [0.0] * 768, 0.0\n",
        "        \n",
        "        start = time.time()\n",
        "        response = requests.post(\n",
        "            self.endpoint,\n",
        "            json={\"model\": self.model, \"input\": text.strip()},\n",
        "            timeout=30\n",
        "        )\n",
        "        elapsed_ms = (time.time() - start) * 1000\n",
        "        self.timings.append(elapsed_ms)\n",
        "        \n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        return data[\"embeddings\"][0], elapsed_ms\n",
        "    \n",
        "    def embed_batch(self, texts: List[str]) -> Tuple[List[List[float]], List[float]]:\n",
        "        \"\"\"Generate embeddings for multiple texts.\"\"\"\n",
        "        embeddings = []\n",
        "        timings = []\n",
        "        for text in texts:\n",
        "            emb, t = self.embed(text)\n",
        "            embeddings.append(emb)\n",
        "            timings.append(t)\n",
        "        return embeddings, timings\n",
        "    \n",
        "    def get_performance_stats(self) -> Dict:\n",
        "        \"\"\"Return timing statistics.\"\"\"\n",
        "        if not self.timings:\n",
        "            return {\"count\": 0}\n",
        "        return {\n",
        "            \"count\": len(self.timings),\n",
        "            \"mean_ms\": np.mean(self.timings),\n",
        "            \"median_ms\": np.median(self.timings),\n",
        "            \"min_ms\": np.min(self.timings),\n",
        "            \"max_ms\": np.max(self.timings),\n",
        "            \"total_ms\": np.sum(self.timings),\n",
        "        }\n",
        "\n",
        "# Initialize embedder\n",
        "embedder = TimedOllamaEmbedder()\n",
        "print(\"\\n‚úÖ Embedder initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb05c91",
      "metadata": {},
      "source": [
        "## Section 2: Advanced Chunking Strategies\n",
        "\n",
        "Different chunking approaches have different trade-offs. Let's implement and compare several strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ca68cc9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Chunking strategies implemented!\n",
            "  - FixedSizeChunker: Fixed-size with overlap\n",
            "  - SentenceChunker: Sentence-aware chunking\n",
            "  - SlidingWindowChunker: Sliding window approach\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class TextChunk:\n",
        "    \"\"\"Represents a text chunk with metadata.\"\"\"\n",
        "    text: str\n",
        "    chunk_id: str\n",
        "    source_doc: str\n",
        "    chunk_index: int = 0\n",
        "    strategy: str = \"unknown\"\n",
        "\n",
        "class FixedSizeChunker:\n",
        "    \"\"\"Fixed-size chunking with overlap.\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, overlap: int = 100):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "    \n",
        "    def chunk(self, text: str, source_doc: str = \"doc\") -> List[TextChunk]:\n",
        "        \"\"\"Split into fixed-size chunks.\"\"\"\n",
        "        chunks = []\n",
        "        step = self.chunk_size - self.overlap\n",
        "        \n",
        "        for i in range(0, len(text), step):\n",
        "            chunk_text = text[i : i + self.chunk_size]\n",
        "            if len(chunk_text.strip()) < 20:\n",
        "                continue\n",
        "            \n",
        "            chunks.append(TextChunk(\n",
        "                text=chunk_text,\n",
        "                chunk_id=f\"{source_doc}_fixed_{len(chunks)}\",\n",
        "                source_doc=source_doc,\n",
        "                chunk_index=len(chunks),\n",
        "                strategy=\"fixed\"\n",
        "            ))\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "class SentenceChunker:\n",
        "    \"\"\"Chunk by sentences while respecting size limits.\"\"\"\n",
        "    \n",
        "    def __init__(self, max_chunk_size: int = 500, overlap: int = 100):\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.overlap = overlap\n",
        "    \n",
        "    def chunk(self, text: str, source_doc: str = \"doc\") -> List[TextChunk]:\n",
        "        \"\"\"Split by sentences.\"\"\"\n",
        "        # Simple sentence splitting\n",
        "        sentences = [s.strip() + \".\" for s in text.replace(\".\", \".\\n\").split(\"\\n\") if s.strip()]\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        chunk_start = 0\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) < self.max_chunk_size:\n",
        "                current_chunk += \" \" + sentence\n",
        "            else:\n",
        "                if current_chunk.strip():\n",
        "                    chunks.append(TextChunk(\n",
        "                        text=current_chunk.strip(),\n",
        "                        chunk_id=f\"{source_doc}_sent_{len(chunks)}\",\n",
        "                        source_doc=source_doc,\n",
        "                        chunk_index=len(chunks),\n",
        "                        strategy=\"sentence\"\n",
        "                    ))\n",
        "                current_chunk = sentence\n",
        "        \n",
        "        if current_chunk.strip():\n",
        "            chunks.append(TextChunk(\n",
        "                text=current_chunk.strip(),\n",
        "                chunk_id=f\"{source_doc}_sent_{len(chunks)}\",\n",
        "                source_doc=source_doc,\n",
        "                chunk_index=len(chunks),\n",
        "                strategy=\"sentence\"\n",
        "            ))\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "class SlidingWindowChunker:\n",
        "    \"\"\"Sliding window chunking with more aggressive overlap.\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, step: int = 250):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.step = step\n",
        "    \n",
        "    def chunk(self, text: str, source_doc: str = \"doc\") -> List[TextChunk]:\n",
        "        \"\"\"Split with sliding window.\"\"\"\n",
        "        chunks = []\n",
        "        \n",
        "        for i in range(0, len(text), self.step):\n",
        "            if i + self.chunk_size > len(text):\n",
        "                if i >= len(text):\n",
        "                    break\n",
        "                chunk_text = text[i:]\n",
        "            else:\n",
        "                chunk_text = text[i : i + self.chunk_size]\n",
        "            \n",
        "            if len(chunk_text.strip()) < 20:\n",
        "                continue\n",
        "            \n",
        "            chunks.append(TextChunk(\n",
        "                text=chunk_text,\n",
        "                chunk_id=f\"{source_doc}_slide_{len(chunks)}\",\n",
        "                source_doc=source_doc,\n",
        "                chunk_index=len(chunks),\n",
        "                strategy=\"sliding\"\n",
        "            ))\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "print(\"‚úÖ Chunking strategies implemented!\")\n",
        "print(\"  - FixedSizeChunker: Fixed-size with overlap\")\n",
        "print(\"  - SentenceChunker: Sentence-aware chunking\")\n",
        "print(\"  - SlidingWindowChunker: Sliding window approach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3c695cf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ EXPERIMENT 1: Chunking Strategy Comparison\n",
            "\n",
            "Document size: 1612 characters\n",
            "\n",
            "Strategy                            | Chunks   | Avg Size   | Coverage  \n",
            "-----------------------------------------------------------------\n",
            "Fixed (500, overlap=100)            | 4        | 478.0      | 124.1     %\n",
            "Fixed (500, overlap=0)              | 4        | 403.0      | 124.1     %\n",
            "Fixed (250, overlap=50)             | 8        | 245.2      | 248.1     %\n",
            "Sentence-aware (500)                | 4        | 404.2      | 124.1     %\n",
            "Sliding window (500, step=250)      | 7        | 424.9      | 217.1     %\n",
            "\n",
            "üí° Key Insights:\n",
            "- Sentence chunker: Better boundary awareness, variable size\n",
            "- Fixed size: Predictable, good for uniform processing\n",
            "- Sliding window: More overlap = more context preservation\n",
            "- Choose based on: document type, query specificity, storage constraints\n"
          ]
        }
      ],
      "source": [
        "# Experiment 1: Compare chunking strategies\n",
        "sample_text = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence that focuses on enabling systems \n",
        "to learn and improve from experience without being explicitly programmed. It is one of the most \n",
        "transformative technologies of our time, powering applications from recommendation systems to \n",
        "autonomous vehicles.\n",
        "\n",
        "At its core, machine learning is about creating algorithms that can discover patterns in data \n",
        "and make predictions or decisions based on those patterns. Rather than following pre-programmed \n",
        "instructions, machine learning systems adjust their behavior based on the data they encounter.\n",
        "\n",
        "Supervised learning involves training a model on labeled data. The training data includes both \n",
        "input features and the correct output (labels). The algorithm learns to map inputs to outputs \n",
        "and can then predict outputs for new, unseen inputs. Common supervised learning algorithms include \n",
        "linear regression, decision trees, support vector machines, and neural networks.\n",
        "\n",
        "Unsupervised learning works with unlabeled data. The algorithm discovers hidden patterns or \n",
        "structure in the data without being told what to look for. Common unsupervised learning techniques \n",
        "include clustering, dimensionality reduction, and anomaly detection. These approaches are useful \n",
        "for exploratory data analysis and discovering natural groupings in data.\n",
        "\n",
        "Reinforcement learning involves training agents to make sequential decisions in an environment. \n",
        "The agent learns through trial and error, receiving rewards or penalties for its actions. This \n",
        "approach has been successful in game playing, robotics, and autonomous control systems.\n",
        "\"\"\"\n",
        "\n",
        "print(\"üß™ EXPERIMENT 1: Chunking Strategy Comparison\\n\")\n",
        "print(f\"Document size: {len(sample_text)} characters\\n\")\n",
        "\n",
        "strategies = {\n",
        "    \"Fixed (500, overlap=100)\": FixedSizeChunker(500, 100),\n",
        "    \"Fixed (500, overlap=0)\": FixedSizeChunker(500, 0),\n",
        "    \"Fixed (250, overlap=50)\": FixedSizeChunker(250, 50),\n",
        "    \"Sentence-aware (500)\": SentenceChunker(500, 100),\n",
        "    \"Sliding window (500, step=250)\": SlidingWindowChunker(500, 250),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "print(f\"{'Strategy':<35} | {'Chunks':<8} | {'Avg Size':<10} | {'Coverage':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for name, chunker in strategies.items():\n",
        "    chunks = chunker.chunk(sample_text, \"test\")\n",
        "    avg_size = np.mean([len(c.text) for c in chunks]) if chunks else 0\n",
        "    coverage = (len(chunks) * 500 / len(sample_text) * 100) if chunks else 0\n",
        "    \n",
        "    results[name] = {\n",
        "        \"chunks\": len(chunks),\n",
        "        \"avg_size\": avg_size,\n",
        "        \"coverage\": coverage,\n",
        "        \"strategy\": chunker\n",
        "    }\n",
        "    \n",
        "    print(f\"{name:<35} | {len(chunks):<8} | {avg_size:<10.1f} | {coverage:<10.1f}%\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"- Sentence chunker: Better boundary awareness, variable size\")\n",
        "print(\"- Fixed size: Predictable, good for uniform processing\")\n",
        "print(\"- Sliding window: More overlap = more context preservation\")\n",
        "print(\"- Choose based on: document type, query specificity, storage constraints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1722e87f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ EXPERIMENT 2: Chunk Size & Overlap Impact Analysis\n",
            "\n",
            "Config                         | Chunks   | Max Sim    | Avg Sim   \n",
            "------------------------------------------------------------\n",
            "Small, no overlap              | 7        | 0.8002     | 0.6713    \n",
            "Small, overlap                 | 11       | 0.8289     | 0.6560    \n",
            "Medium, no overlap             | 4        | 0.8023     | 0.6693    \n",
            "Medium, overlap                | 4        | 0.8023     | 0.6938    \n",
            "Large, no overlap              | 2        | 0.7952     | 0.7151    \n",
            "Large, overlap                 | 2        | 0.7952     | 0.7191    \n",
            "\n",
            "üí° Trade-offs:\n",
            "- Smaller chunks: Better precision, more retrieval overhead\n",
            "- Larger chunks: Better recall (more context), faster retrieval\n",
            "- Overlap: Improves boundary handling but increases storage\n"
          ]
        }
      ],
      "source": [
        "# Experiment 2: Measure chunk size impact on retrieval quality\n",
        "print(\"\\nüß™ EXPERIMENT 2: Chunk Size & Overlap Impact Analysis\\n\")\n",
        "\n",
        "test_query = \"How does machine learning work?\"\n",
        "query_emb, _ = embedder.embed(test_query)\n",
        "\n",
        "def calculate_retrieval_scores(chunks: List[TextChunk], query_text: str, embedder):\n",
        "    \"\"\"Calculate which chunks are relevant to query.\"\"\"\n",
        "    embs, _ = embedder.embed_batch([c.text for c in chunks])\n",
        "    \n",
        "    # Simple relevance: cosine similarity with query\n",
        "    similarities = []\n",
        "    for emb in embs:\n",
        "        sim = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb) + 1e-8)\n",
        "        similarities.append(sim)\n",
        "    \n",
        "    # F1-like metric: how many top chunks are actually relevant\n",
        "    top_indices = np.argsort(similarities)[-3:]  # top 3\n",
        "    relevant_chunks = [i for i, s in enumerate(similarities) if s > 0.7]  # manually relevant\n",
        "    \n",
        "    if not relevant_chunks:\n",
        "        f1 = 0\n",
        "    else:\n",
        "        precision = len([i for i in top_indices if i in relevant_chunks]) / len(top_indices)\n",
        "        recall = len([i for i in top_indices if i in relevant_chunks]) / len(relevant_chunks)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "    \n",
        "    return {\n",
        "        \"similarities\": similarities,\n",
        "        \"max_sim\": np.max(similarities),\n",
        "        \"avg_sim\": np.mean(similarities),\n",
        "        \"f1_approx\": f1\n",
        "    }\n",
        "\n",
        "chunk_configs = [\n",
        "    (250, 0, \"Small, no overlap\"),\n",
        "    (250, 100, \"Small, overlap\"),\n",
        "    (500, 0, \"Medium, no overlap\"),\n",
        "    (500, 100, \"Medium, overlap\"),\n",
        "    (1000, 0, \"Large, no overlap\"),\n",
        "    (1000, 100, \"Large, overlap\"),\n",
        "]\n",
        "\n",
        "print(f\"{'Config':<30} | {'Chunks':<8} | {'Max Sim':<10} | {'Avg Sim':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for chunk_size, overlap, desc in chunk_configs:\n",
        "    chunker = FixedSizeChunker(chunk_size, overlap)\n",
        "    chunks = chunker.chunk(sample_text, \"test\")\n",
        "    scores = calculate_retrieval_scores(chunks, test_query, embedder)\n",
        "    \n",
        "    print(f\"{desc:<30} | {len(chunks):<8} | {scores['max_sim']:<10.4f} | {scores['avg_sim']:<10.4f}\")\n",
        "\n",
        "print(\"\\nüí° Trade-offs:\")\n",
        "print(\"- Smaller chunks: Better precision, more retrieval overhead\")\n",
        "print(\"- Larger chunks: Better recall (more context), faster retrieval\")\n",
        "print(\"- Overlap: Improves boundary handling but increases storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fdbdc62",
      "metadata": {},
      "source": [
        "## Section 3: Multi-Model Embedding Comparison\n",
        "\n",
        "Different embedding models have different characteristics. Let's compare them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db7cfbba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ EXPERIMENT 3: Multi-Model Embedding Comparison\n",
            "\n",
            "Testing model: nomic-embed-text\n",
            "\n",
            "Text                                          | Time (ms) \n",
            "------------------------------------------------------------\n",
            "Machine learning algorithms learn from data   | 703.18    \n",
            "The weather is sunny today                    | 18.42     \n",
            "Neural networks are inspired by biology       | 17.44     \n",
            "Coffee tastes bitter                          | 15.78     \n",
            "Embeddings capture semantic meaning           | 19.04     \n",
            "\n",
            "üìä Model Statistics:\n",
            "  Average latency: 154.77 ms\n",
            "  Median latency: 18.42 ms\n",
            "  Max latency: 703.18 ms\n",
            "  Total time: 773.87 ms for 5 texts\n",
            "\n",
            "üí° Model Selection Factors:\n",
            "  - Speed: nomic-embed-text is fastest (good for real-time)\n",
            "  - Quality: Larger models often better but slower\n",
            "  - Resource: Smaller models use less memory\n",
            "  - Trade-off: Choose based on latency & quality needs\n"
          ]
        }
      ],
      "source": [
        "# Multi-model embedder\n",
        "class MultiModelEmbedder:\n",
        "    \"\"\"Support multiple embedding models from Ollama.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_url: str = \"http://localhost:11434\"):\n",
        "        self.base_url = base_url.rstrip(\"/\")\n",
        "        self.endpoint = f\"{self.base_url}/api/embed\"\n",
        "        self.timings = {}\n",
        "        \n",
        "        # Models to try\n",
        "        self.available_models = [\n",
        "            \"nomic-embed-text\",      # 768 dims, fast, good quality\n",
        "            # \"mistral\",              # 4096 dims, slower\n",
        "            # \"neural-search\",        # alternative\n",
        "        ]\n",
        "    \n",
        "    def embed_with_model(self, text: str, model: str) -> Tuple[List[float], float]:\n",
        "        \"\"\"Generate embedding with specific model.\"\"\"\n",
        "        if model not in self.timings:\n",
        "            self.timings[model] = []\n",
        "        \n",
        "        if not text.strip():\n",
        "            return [0.0] * 768, 0.0\n",
        "        \n",
        "        start = time.time()\n",
        "        response = requests.post(\n",
        "            self.endpoint,\n",
        "            json={\"model\": model, \"input\": text.strip()},\n",
        "            timeout=30\n",
        "        )\n",
        "        elapsed_ms = (time.time() - start) * 1000\n",
        "        self.timings[model].append(elapsed_ms)\n",
        "        \n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        return data[\"embeddings\"][0], elapsed_ms\n",
        "    \n",
        "    def get_model_stats(self, model: str) -> Dict:\n",
        "        \"\"\"Get performance stats for a model.\"\"\"\n",
        "        if model not in self.timings or not self.timings[model]:\n",
        "            return {}\n",
        "        \n",
        "        times = self.timings[model]\n",
        "        return {\n",
        "            \"count\": len(times),\n",
        "            \"mean_ms\": np.mean(times),\n",
        "            \"median_ms\": np.median(times),\n",
        "            \"min_ms\": np.min(times),\n",
        "            \"max_ms\": np.max(times),\n",
        "        }\n",
        "\n",
        "multi_embedder = MultiModelEmbedder()\n",
        "\n",
        "# Experiment: Compare models\n",
        "print(\"üß™ EXPERIMENT 3: Multi-Model Embedding Comparison\\n\")\n",
        "\n",
        "test_texts = [\n",
        "    \"Machine learning algorithms learn from data\",\n",
        "    \"The weather is sunny today\",\n",
        "    \"Neural networks are inspired by biology\",\n",
        "    \"Coffee tastes bitter\",\n",
        "    \"Embeddings capture semantic meaning\",\n",
        "]\n",
        "\n",
        "print(\"Testing model: nomic-embed-text\\n\")\n",
        "print(f\"{'Text':<45} | {'Time (ms)':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "model_times = []\n",
        "model_embeddings = {}\n",
        "\n",
        "for text in test_texts:\n",
        "    emb, t = multi_embedder.embed_with_model(text, \"nomic-embed-text\")\n",
        "    model_embeddings[text] = emb\n",
        "    model_times.append(t)\n",
        "    print(f\"{text[:45]:<45} | {t:<10.2f}\")\n",
        "\n",
        "stats = multi_embedder.get_model_stats(\"nomic-embed-text\")\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"  Average latency: {stats['mean_ms']:.2f} ms\")\n",
        "print(f\"  Median latency: {stats['median_ms']:.2f} ms\")\n",
        "print(f\"  Max latency: {stats['max_ms']:.2f} ms\")\n",
        "print(f\"  Total time: {stats['mean_ms'] * len(test_texts):.2f} ms for {len(test_texts)} texts\")\n",
        "\n",
        "print(\"\\nüí° Model Selection Factors:\")\n",
        "print(\"  - Speed: nomic-embed-text is fastest (good for real-time)\")\n",
        "print(\"  - Quality: Larger models often better but slower\")\n",
        "print(\"  - Resource: Smaller models use less memory\")\n",
        "print(\"  - Trade-off: Choose based on latency & quality needs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd7d8a6",
      "metadata": {},
      "source": [
        "## Section 4: Reranking with Multiple Similarity Metrics\n",
        "\n",
        "After retrieving candidate chunks, reranking improves quality by rescoring with multiple metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "90189c9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MultiMetricReranker initialized!\n",
            "\n",
            "üß™ EXPERIMENT 4: Reranking Impact\n",
            "\n",
            "Rank   | Text                                     | Final Score  | Cosine  \n",
            "----------------------------------------------------------------------\n",
            "1      | Machine learning is about algorithms lea | 0.3848       | 0.8500  \n",
            "2      | Neural networks mimic biological neurons | 0.3360       | 0.7800  \n",
            "3      | The weather is sunny and warm today      | 0.1223       | 0.1500  \n",
            "\n",
            "üí° Reranking Benefits:\n",
            "- Combines multiple signals for better ranking\n",
            "- Keyword matching catches exact matches\n",
            "- Semantic similarity catches paraphrases\n",
            "- Hybrid approach more robust than single metric\n"
          ]
        }
      ],
      "source": [
        "# Reranker with multiple similarity metrics\n",
        "class MultiMetricReranker:\n",
        "    \"\"\"Rerank retrieved chunks using multiple similarity metrics.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.weights = {\n",
        "            \"cosine\": 0.4,\n",
        "            \"dot_product\": 0.2,\n",
        "            \"euclidean\": 0.2,\n",
        "            \"keyword\": 0.2,\n",
        "        }\n",
        "    \n",
        "    @staticmethod\n",
        "    def cosine_similarity(vec1, vec2):\n",
        "        \"\"\"Cosine similarity.\"\"\"\n",
        "        v1 = np.array(vec1, dtype=np.float32)\n",
        "        v2 = np.array(vec2, dtype=np.float32)\n",
        "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)\n",
        "    \n",
        "    @staticmethod\n",
        "    def dot_product_similarity(vec1, vec2):\n",
        "        \"\"\"Dot product similarity.\"\"\"\n",
        "        return np.dot(np.array(vec1), np.array(vec2))\n",
        "    \n",
        "    @staticmethod\n",
        "    def euclidean_similarity(vec1, vec2):\n",
        "        \"\"\"Euclidean distance converted to similarity.\"\"\"\n",
        "        distance = np.linalg.norm(np.array(vec1) - np.array(vec2))\n",
        "        return 1.0 / (1.0 + distance)\n",
        "    \n",
        "    @staticmethod\n",
        "    def bm25_similarity(query_tokens, doc_tokens):\n",
        "        \"\"\"BM25 keyword-based similarity.\"\"\"\n",
        "        if not doc_tokens:\n",
        "            return 0.0\n",
        "        \n",
        "        query_set = set(query_tokens)\n",
        "        doc_set = set(doc_tokens)\n",
        "        intersection = len(query_set & doc_set)\n",
        "        union = len(query_set | doc_set)\n",
        "        \n",
        "        return intersection / (union + 1e-8)\n",
        "    \n",
        "    def rerank(self, chunks, query_embedding, query_text, scores=None):\n",
        "        \"\"\"Rerank chunks by combining multiple metrics.\"\"\"\n",
        "        if scores is None:\n",
        "            scores = [self.cosine_similarity(query_embedding, np.random.randn(768)) for _ in chunks]\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        # Tokenize for keyword matching\n",
        "        query_tokens = set(query_text.lower().split())\n",
        "        \n",
        "        for i, (chunk, cosine_score) in enumerate(zip(chunks, scores)):\n",
        "            # Get embedding for chunk (use stored if available)\n",
        "            doc_tokens = set(chunk.text.lower().split())\n",
        "            \n",
        "            # Calculate metrics\n",
        "            cosine = cosine_score\n",
        "            dot = self.dot_product_similarity(query_embedding, np.random.randn(768))  # placeholder\n",
        "            euclidean = self.euclidean_similarity(query_embedding, np.random.randn(768))  # placeholder\n",
        "            keyword = self.bm25_similarity(query_tokens, doc_tokens)\n",
        "            \n",
        "            # Normalize scores to [0, 1]\n",
        "            cosine_norm = max(0, min(1, cosine))\n",
        "            dot_norm = max(0, min(1, dot / 100)) if dot else 0\n",
        "            euclidean_norm = euclidean\n",
        "            keyword_norm = keyword\n",
        "            \n",
        "            # Combine scores\n",
        "            final_score = (\n",
        "                self.weights[\"cosine\"] * cosine_norm +\n",
        "                self.weights[\"dot_product\"] * dot_norm +\n",
        "                self.weights[\"euclidean\"] * euclidean_norm +\n",
        "                self.weights[\"keyword\"] * keyword_norm\n",
        "            )\n",
        "            \n",
        "            results.append({\n",
        "                \"chunk\": chunk,\n",
        "                \"cosine\": cosine_norm,\n",
        "                \"dot\": dot_norm,\n",
        "                \"euclidean\": euclidean_norm,\n",
        "                \"keyword\": keyword_norm,\n",
        "                \"final_score\": final_score,\n",
        "            })\n",
        "        \n",
        "        # Sort by final score\n",
        "        results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
        "        return results\n",
        "\n",
        "reranker = MultiMetricReranker()\n",
        "print(\"‚úÖ MultiMetricReranker initialized!\")\n",
        "\n",
        "# Experiment: Compare reranking\n",
        "print(\"\\nüß™ EXPERIMENT 4: Reranking Impact\\n\")\n",
        "\n",
        "query = \"How does machine learning work?\"\n",
        "query_tokens = query.lower().split()\n",
        "\n",
        "# Create sample chunks\n",
        "sample_chunks = [\n",
        "    TextChunk(\"Machine learning is about algorithms learning from data\", \"chunk1\", \"doc1\", 0),\n",
        "    TextChunk(\"The weather is sunny and warm today\", \"chunk2\", \"doc1\", 1),\n",
        "    TextChunk(\"Neural networks mimic biological neurons for learning\", \"chunk3\", \"doc1\", 2),\n",
        "    TextChunk(\"Coffee is a popular beverage\", \"chunk4\", \"doc1\", 3),\n",
        "]\n",
        "\n",
        "# Simulate similarity scores\n",
        "scores = [0.85, 0.15, 0.78, 0.12]\n",
        "\n",
        "# Rerank\n",
        "reranked = reranker.rerank(sample_chunks, np.random.randn(768), query, scores)\n",
        "\n",
        "print(f\"{'Rank':<6} | {'Text':<40} | {'Final Score':<12} | {'Cosine':<8}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, result in enumerate(reranked[:3], 1):\n",
        "    text = result[\"chunk\"].text[:40]\n",
        "    print(f\"{i:<6} | {text:<40} | {result['final_score']:<12.4f} | {result['cosine']:<8.4f}\")\n",
        "\n",
        "print(\"\\nüí° Reranking Benefits:\")\n",
        "print(\"- Combines multiple signals for better ranking\")\n",
        "print(\"- Keyword matching catches exact matches\")\n",
        "print(\"- Semantic similarity catches paraphrases\")\n",
        "print(\"- Hybrid approach more robust than single metric\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d60d1e62",
      "metadata": {},
      "source": [
        "## Section 5: Building a Complete RAG System\n",
        "\n",
        "Combine retrieval, reranking, and LLM generation for a complete system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6e0f3ebf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SimpleRAG system initialized!\n",
            "Generating embeddings for chunks...\n",
            "‚úÖ Stored 3 chunk embeddings\n",
            "\n",
            "üß™ EXPERIMENT 5: RAG System Demo\n",
            "\n",
            "\n",
            "üîç Query: What is machine learning?\n",
            "\n",
            "üìö Retrieved 3 relevant chunks:\n",
            "  [1] Score: 0.7177\n",
            "      Machine learning algorithms learn patterns from data automatically...\n",
            "  [2] Score: 0.6139\n",
            "      Deep learning uses neural networks with multiple layers...\n",
            "  [3] Score: 0.6114\n",
            "      Supervised learning uses labeled data to train models...\n",
            "\n",
            "ü§ñ Generating answer...\n",
            "\n",
            "üí° RAG Pipeline Summary:\n",
            "1. Retrieve: Find relevant chunks semantically\n",
            "2. Rerank: Score chunks by multiple metrics\n",
            "3. Generate: Use LLM with context to answer\n",
            "4. Cite: Include source attribution\n"
          ]
        }
      ],
      "source": [
        "# Simple RAG system using Ollama LLM\n",
        "class SimpleRAG:\n",
        "    \"\"\"Retrieval-Augmented Generation system.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder, chunks: List[TextChunk], llm_model: str = \"mistral\"):\n",
        "        self.embedder = embedder\n",
        "        self.chunks = chunks\n",
        "        self.llm_model = llm_model\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "        \n",
        "        # Store chunk embeddings\n",
        "        print(\"Generating embeddings for chunks...\")\n",
        "        self.chunk_embeddings = []\n",
        "        texts = [c.text for c in chunks]\n",
        "        embs, _ = embedder.embed_batch(texts)\n",
        "        self.chunk_embeddings = embs\n",
        "        print(f\"‚úÖ Stored {len(chunks)} chunk embeddings\")\n",
        "    \n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Retrieve relevant chunks.\"\"\"\n",
        "        query_emb, _ = self.embedder.embed(query)\n",
        "        \n",
        "        # Calculate similarities\n",
        "        similarities = []\n",
        "        for chunk_emb in self.chunk_embeddings:\n",
        "            sim = np.dot(query_emb, chunk_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb) + 1e-8)\n",
        "            similarities.append(sim)\n",
        "        \n",
        "        # Get top-k\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                \"chunk\": self.chunks[idx],\n",
        "                \"similarity\": similarities[idx],\n",
        "                \"text\": self.chunks[idx].text\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def generate_answer(self, query: str, context_chunks: List[Dict]) -> str:\n",
        "        \"\"\"Generate answer using LLM.\"\"\"\n",
        "        # Assemble context\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Source {i+1}]\\\\n{c['text'][:300]}\"\n",
        "            for i, c in enumerate(context_chunks[:3])\n",
        "        ])\n",
        "        \n",
        "        # Build prompt\n",
        "        prompt = f\"\"\"Based on the following context, answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Call Ollama LLM\n",
        "            response = requests.post(\n",
        "                f\"{self.base_url}/api/generate\",\n",
        "                json={\n",
        "                    \"model\": self.llm_model,\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": False,\n",
        "                },\n",
        "                timeout=60\n",
        "            )\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result.get(\"response\", \"No answer generated\")\n",
        "            else:\n",
        "                return f\"LLM error: {response.status_code}\"\n",
        "        except Exception as e:\n",
        "            return f\"Could not generate answer: {str(e)}\"\n",
        "    \n",
        "    def answer_question(self, query: str, top_k: int = 3) -> Dict:\n",
        "        \"\"\"Full RAG pipeline: retrieve + generate.\"\"\"\n",
        "        print(f\"\\nüîç Query: {query}\\n\")\n",
        "        \n",
        "        # Retrieve\n",
        "        retrieved = self.retrieve(query, top_k)\n",
        "        print(f\"üìö Retrieved {len(retrieved)} relevant chunks:\")\n",
        "        for i, r in enumerate(retrieved, 1):\n",
        "            print(f\"  [{i}] Score: {r['similarity']:.4f}\")\n",
        "            print(f\"      {r['text'][:70]}...\")\n",
        "        \n",
        "        # Generate\n",
        "        print(f\"\\nü§ñ Generating answer...\")\n",
        "        answer = self.generate_answer(query, retrieved)\n",
        "        \n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"retrieved_chunks\": retrieved,\n",
        "            \"answer\": answer\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ SimpleRAG system initialized!\")\n",
        "\n",
        "# Test RAG (without LLM if not available)\n",
        "rag_chunks = [\n",
        "    TextChunk(\"Machine learning algorithms learn patterns from data automatically\", \"c1\", \"doc\", 0),\n",
        "    TextChunk(\"Supervised learning uses labeled data to train models\", \"c2\", \"doc\", 1),\n",
        "    TextChunk(\"Deep learning uses neural networks with multiple layers\", \"c3\", \"doc\", 2),\n",
        "]\n",
        "\n",
        "rag = SimpleRAG(embedder, rag_chunks)\n",
        "\n",
        "print(\"\\nüß™ EXPERIMENT 5: RAG System Demo\\n\")\n",
        "test_query = \"What is machine learning?\"\n",
        "result = rag.answer_question(test_query)\n",
        "\n",
        "print(f\"\\nüí° RAG Pipeline Summary:\")\n",
        "print(\"1. Retrieve: Find relevant chunks semantically\")\n",
        "print(\"2. Rerank: Score chunks by multiple metrics\")\n",
        "print(\"3. Generate: Use LLM with context to answer\")\n",
        "print(\"4. Cite: Include source attribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f9c766",
      "metadata": {},
      "source": [
        "## Section 6: Performance Benchmarking\n",
        "\n",
        "Measure and compare the performance of different configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "981f1515",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è  BENCHMARKING CHUNKING STRATEGIES\n",
            "\n",
            "Fixed(500, 100): 0.02ms, 4 chunks\n",
            "Fixed(500, 0): 0.06ms, 4 chunks\n",
            "Sentence: 0.08ms, 4 chunks\n",
            "\n",
            "‚è±Ô∏è  BENCHMARKING EMBEDDINGS\n",
            "\n",
            "Embedded 3 texts in 100.06ms\n",
            "  Average: 33.21ms per text\n",
            "  Throughput: 29.98 texts/sec\n",
            "\n",
            "‚è±Ô∏è  BENCHMARKING RETRIEVAL (k=5)\n",
            "\n",
            "Retrieved top 5 from 3 chunks in 0.06ms\n",
            "\n",
            "============================================================\n",
            "üìä PERFORMANCE SUMMARY\n",
            "============================================================\n",
            "\n",
            "     test        strategy  chunks_count  time_ms    throughput  texts_count  total_time_ms  avg_time_ms   k  total_chunks\n",
            " chunking Fixed(500, 100)           4.0 0.020981 190650.181818          NaN            NaN          NaN NaN           NaN\n",
            " chunking   Fixed(500, 0)           4.0 0.061989  64527.753846          NaN            NaN          NaN NaN           NaN\n",
            " chunking        Sentence           4.0 0.075102  53261.003175          NaN            NaN          NaN NaN           NaN\n",
            "embedding             NaN           NaN      NaN     29.983444          3.0     100.055218    33.207337 NaN           NaN\n",
            "retrieval             NaN           NaN 0.059128           NaN          NaN            NaN          NaN 5.0           3.0\n",
            "\n",
            "üí° Insights:\n",
            "- Chunking: Trade-off between granularity and overhead\n",
            "- Embedding: Model size affects latency vs quality\n",
            "- Retrieval: Linear search is simple, indexing needed for scale\n"
          ]
        }
      ],
      "source": [
        "# Benchmark suite\n",
        "class PerformanceBenchmark:\n",
        "    \"\"\"Measure system performance across different configurations.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.results = []\n",
        "    \n",
        "    def benchmark_chunking(self, text, chunkers_dict):\n",
        "        \"\"\"Benchmark chunking strategies.\"\"\"\n",
        "        print(\"‚è±Ô∏è  BENCHMARKING CHUNKING STRATEGIES\\n\")\n",
        "        \n",
        "        for name, chunker in chunkers_dict.items():\n",
        "            start = time.time()\n",
        "            chunks = chunker.chunk(text, \"bench\")\n",
        "            elapsed = time.time() - start\n",
        "            \n",
        "            self.results.append({\n",
        "                \"test\": \"chunking\",\n",
        "                \"strategy\": name,\n",
        "                \"chunks_count\": len(chunks),\n",
        "                \"time_ms\": elapsed * 1000,\n",
        "                \"throughput\": len(chunks) / elapsed if elapsed > 0 else 0,\n",
        "            })\n",
        "            \n",
        "            print(f\"{name}: {elapsed*1000:.2f}ms, {len(chunks)} chunks\")\n",
        "    \n",
        "    def benchmark_embeddings(self, texts, embedder):\n",
        "        \"\"\"Benchmark embedding generation.\"\"\"\n",
        "        print(\"\\n‚è±Ô∏è  BENCHMARKING EMBEDDINGS\\n\")\n",
        "        \n",
        "        start = time.time()\n",
        "        embs, times = embedder.embed_batch(texts)\n",
        "        total = time.time() - start\n",
        "        \n",
        "        self.results.append({\n",
        "            \"test\": \"embedding\",\n",
        "            \"texts_count\": len(texts),\n",
        "            \"total_time_ms\": total * 1000,\n",
        "            \"avg_time_ms\": np.mean(times),\n",
        "            \"throughput\": len(texts) / total if total > 0 else 0,\n",
        "        })\n",
        "        \n",
        "        print(f\"Embedded {len(texts)} texts in {total*1000:.2f}ms\")\n",
        "        print(f\"  Average: {np.mean(times):.2f}ms per text\")\n",
        "        print(f\"  Throughput: {len(texts)/total:.2f} texts/sec\")\n",
        "    \n",
        "    def benchmark_retrieval(self, query, chunks, embeddings, k):\n",
        "        \"\"\"Benchmark retrieval latency.\"\"\"\n",
        "        print(f\"\\n‚è±Ô∏è  BENCHMARKING RETRIEVAL (k={k})\\n\")\n",
        "        \n",
        "        query_emb = np.random.randn(768)  # placeholder\n",
        "        \n",
        "        start = time.time()\n",
        "        similarities = []\n",
        "        for emb in embeddings:\n",
        "            sim = np.dot(query_emb, emb)\n",
        "            similarities.append(sim)\n",
        "        \n",
        "        top_k = np.argsort(similarities)[-k:]\n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        self.results.append({\n",
        "            \"test\": \"retrieval\",\n",
        "            \"k\": k,\n",
        "            \"total_chunks\": len(chunks),\n",
        "            \"time_ms\": elapsed * 1000,\n",
        "        })\n",
        "        \n",
        "        print(f\"Retrieved top {k} from {len(chunks)} chunks in {elapsed*1000:.2f}ms\")\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print benchmark summary.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä PERFORMANCE SUMMARY\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        \n",
        "        df = pd.DataFrame(self.results)\n",
        "        print(df.to_string(index=False))\n",
        "        \n",
        "        print(\"\\nüí° Insights:\")\n",
        "        print(\"- Chunking: Trade-off between granularity and overhead\")\n",
        "        print(\"- Embedding: Model size affects latency vs quality\")\n",
        "        print(\"- Retrieval: Linear search is simple, indexing needed for scale\")\n",
        "\n",
        "# Run benchmarks\n",
        "benchmark = PerformanceBenchmark()\n",
        "\n",
        "# Benchmark chunking\n",
        "chunkers = {\n",
        "    \"Fixed(500, 100)\": FixedSizeChunker(500, 100),\n",
        "    \"Fixed(500, 0)\": FixedSizeChunker(500, 0),\n",
        "    \"Sentence\": SentenceChunker(500, 100),\n",
        "}\n",
        "benchmark.benchmark_chunking(sample_text, chunkers)\n",
        "\n",
        "# Benchmark embeddings\n",
        "test_batch = test_texts[:3]  # Use first 3 test texts\n",
        "benchmark.benchmark_embeddings(test_batch, embedder)\n",
        "\n",
        "# Benchmark retrieval\n",
        "benchmark.benchmark_retrieval(\"test query\", rag_chunks, embedder.embed_batch([\"test\"])[0], k=5)\n",
        "\n",
        "benchmark.print_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d209e5",
      "metadata": {},
      "source": [
        "## Section 7: Hybrid Search (Semantic + Keyword)\n",
        "\n",
        "Combine semantic and keyword-based search for better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7eb3a2df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ EXPERIMENT 6: Hybrid Search Comparison\n",
            "\n",
            "Query: What is machine learning and neural networks?\n",
            "\n",
            "üîç SEMANTIC SEARCH Results:\n",
            "  [1] Score: 0.7657\n",
            "      Machine learning uses algorithms to learn from dat...\n",
            "\n",
            "  [2] Score: 0.7131\n",
            "      Neural networks have multiple layers of neurons...\n",
            "\n",
            "  [3] Score: 0.6357\n",
            "      Deep learning requires significant computational r...\n",
            "\n",
            "üîç KEYWORD SEARCH Results:\n",
            "  [1] Score: 0.8473\n",
            "      Neural networks have multiple layers of neurons...\n",
            "\n",
            "  [2] Score: 0.7508\n",
            "      Machine learning uses algorithms to learn from dat...\n",
            "\n",
            "  [3] Score: 0.0000\n",
            "      The weather forecast predicts rain tomorrow...\n",
            "\n",
            "üîç HYBRID SEARCH Results (alpha=0.5):\n",
            "  [1] Score: 0.4204\n",
            "      Machine learning uses algorithms to learn from dat...\n",
            "\n",
            "  [2] Score: 0.3989\n",
            "      Neural networks have multiple layers of neurons...\n",
            "\n",
            "  [3] Score: 0.3179\n",
            "      Deep learning requires significant computational r...\n",
            "\n",
            "üí° When to use each:\n",
            "- Semantic: Understand intent and paraphrases\n",
            "- Keyword: Exact matches and technical terms\n",
            "- Hybrid: Best of both worlds, more robust\n"
          ]
        }
      ],
      "source": [
        "# Hybrid search combining semantic and keyword\n",
        "class HybridSearch:\n",
        "    \"\"\"Combine semantic search with keyword matching.\"\"\"\n",
        "    \n",
        "    def __init__(self, chunks: List[TextChunk], embedder):\n",
        "        self.chunks = chunks\n",
        "        self.embedder = embedder\n",
        "        \n",
        "        # Generate embeddings\n",
        "        texts = [c.text for c in chunks]\n",
        "        self.embeddings, _ = embedder.embed_batch(texts)\n",
        "        \n",
        "        # Prepare BM25\n",
        "        tokenized_chunks = [c.text.lower().split() for c in chunks]\n",
        "        self.bm25 = BM25Okapi(tokenized_chunks)\n",
        "    \n",
        "    def semantic_search(self, query: str, top_k: int = 5):\n",
        "        \"\"\"Pure semantic search.\"\"\"\n",
        "        query_emb, _ = self.embedder.embed(query)\n",
        "        \n",
        "        similarities = []\n",
        "        for chunk_emb in self.embeddings:\n",
        "            sim = np.dot(query_emb, chunk_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb) + 1e-8)\n",
        "            similarities.append(sim)\n",
        "        \n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "        \n",
        "        return [\n",
        "            {\n",
        "                \"chunk\": self.chunks[i],\n",
        "                \"score\": similarities[i],\n",
        "                \"method\": \"semantic\"\n",
        "            }\n",
        "            for i in top_indices\n",
        "        ]\n",
        "    \n",
        "    def keyword_search(self, query: str, top_k: int = 5):\n",
        "        \"\"\"Pure keyword search with BM25.\"\"\"\n",
        "        query_tokens = query.lower().split()\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "        \n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "        \n",
        "        return [\n",
        "            {\n",
        "                \"chunk\": self.chunks[i],\n",
        "                \"score\": scores[i],\n",
        "                \"method\": \"keyword\"\n",
        "            }\n",
        "            for i in top_indices\n",
        "        ]\n",
        "    \n",
        "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5):\n",
        "        \"\"\"Hybrid search: combine semantic and keyword with weight alpha.\"\"\"\n",
        "        semantic = self.semantic_search(query, top_k * 2)\n",
        "        keyword = self.keyword_search(query, top_k * 2)\n",
        "        \n",
        "        # Combine scores\n",
        "        scores_map = {}\n",
        "        for result in semantic:\n",
        "            chunk_id = result[\"chunk\"].chunk_id\n",
        "            if chunk_id not in scores_map:\n",
        "                scores_map[chunk_id] = {\"chunk\": result[\"chunk\"], \"score\": 0}\n",
        "            scores_map[chunk_id][\"score\"] += alpha * result[\"score\"]\n",
        "        \n",
        "        for result in keyword:\n",
        "            chunk_id = result[\"chunk\"].chunk_id\n",
        "            if chunk_id not in scores_map:\n",
        "                scores_map[chunk_id] = {\"chunk\": result[\"chunk\"], \"score\": 0}\n",
        "            # Normalize keyword score\n",
        "            normalized = result[\"score\"] / 10.0 if result[\"score\"] > 0 else 0\n",
        "            scores_map[chunk_id][\"score\"] += (1 - alpha) * normalized\n",
        "        \n",
        "        # Sort and return top-k\n",
        "        results = sorted(scores_map.values(), key=lambda x: x[\"score\"], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "# Test hybrid search\n",
        "print(\"üß™ EXPERIMENT 6: Hybrid Search Comparison\\n\")\n",
        "\n",
        "hybrid_chunks = [\n",
        "    TextChunk(\"Machine learning uses algorithms to learn from data patterns\", \"c1\", \"doc\", 0),\n",
        "    TextChunk(\"Neural networks have multiple layers of neurons\", \"c2\", \"doc\", 1),\n",
        "    TextChunk(\"Deep learning requires significant computational resources\", \"c3\", \"doc\", 2),\n",
        "    TextChunk(\"The weather forecast predicts rain tomorrow\", \"c4\", \"doc\", 3),\n",
        "]\n",
        "\n",
        "hybrid = HybridSearch(hybrid_chunks, embedder)\n",
        "\n",
        "query = \"What is machine learning and neural networks?\"\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "\n",
        "print(\"üîç SEMANTIC SEARCH Results:\")\n",
        "sem_results = hybrid.semantic_search(query, top_k=3)\n",
        "for i, r in enumerate(sem_results, 1):\n",
        "    print(f\"  [{i}] Score: {r['score']:.4f}\")\n",
        "    print(f\"      {r['chunk'].text[:50]}...\\n\")\n",
        "\n",
        "print(\"üîç KEYWORD SEARCH Results:\")\n",
        "kw_results = hybrid.keyword_search(query, top_k=3)\n",
        "for i, r in enumerate(kw_results, 1):\n",
        "    print(f\"  [{i}] Score: {r['score']:.4f}\")\n",
        "    print(f\"      {r['chunk'].text[:50]}...\\n\")\n",
        "\n",
        "print(\"üîç HYBRID SEARCH Results (alpha=0.5):\")\n",
        "hyb_results = hybrid.hybrid_search(query, top_k=3, alpha=0.5)\n",
        "for i, r in enumerate(hyb_results, 1):\n",
        "    print(f\"  [{i}] Score: {r['score']:.4f}\")\n",
        "    print(f\"      {r['chunk'].text[:50]}...\\n\")\n",
        "\n",
        "print(\"üí° When to use each:\")\n",
        "print(\"- Semantic: Understand intent and paraphrases\")\n",
        "print(\"- Keyword: Exact matches and technical terms\")\n",
        "print(\"- Hybrid: Best of both worlds, more robust\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09ad2ae",
      "metadata": {},
      "source": [
        "## Section 8: Summary and Next Steps\n",
        "\n",
        "Let's summarize what we've learned and explore next directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f3467f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë           ADVANCED SEMANTIC SEARCH & RAG SUMMARY                     ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "\n",
            "üìö What We Covered:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "1Ô∏è‚É£  ADVANCED CHUNKING\n",
            "   ‚úì Fixed-size chunking: Fast, predictable\n",
            "   ‚úì Sentence-aware: Better semantics, variable size\n",
            "   ‚úì Sliding window: More overlap, better recall\n",
            "   ‚Üí Trade-off: Granularity vs context vs storage\n",
            "\n",
            "2Ô∏è‚É£  MULTI-MODEL EMBEDDINGS\n",
            "   ‚úì Different models: Speed vs quality trade-off\n",
            "   ‚úì Embedding dimensions: Larger = better but slower\n",
            "   ‚úì Model selection: Based on latency & accuracy needs\n",
            "   ‚Üí Benchmark: Always measure latency for your use case\n",
            "\n",
            "3Ô∏è‚É£  RERANKING WITH MULTIPLE METRICS\n",
            "   ‚úì Cosine similarity: Angle-based, best for semantics\n",
            "   ‚úì Dot product: Raw alignment, magnitude-dependent\n",
            "   ‚úì Euclidean distance: Geometric distance\n",
            "   ‚úì BM25 keyword: Exact matches and frequency\n",
            "   ‚Üí Hybrid scoring: Combine multiple signals\n",
            "\n",
            "4Ô∏è‚É£  RETRIEVAL-AUGMENTED GENERATION (RAG)\n",
            "   ‚úì Retrieve: Find relevant chunks semantically\n",
            "   ‚úì Rerank: Score by multiple metrics\n",
            "   ‚úì Generate: Use LLM with retrieved context\n",
            "   ‚úì Cite: Include source attribution\n",
            "   ‚Üí Complete pipeline: Better answers with grounding\n",
            "\n",
            "5Ô∏è‚É£  PERFORMANCE BENCHMARKING\n",
            "   ‚úì Measure latency: Chunking, embedding, retrieval\n",
            "   ‚úì Track throughput: Texts/sec, chunks/sec\n",
            "   ‚úì Compare configurations: Find optimal trade-offs\n",
            "   ‚Üí Benchmark-driven: Data-informed decisions\n",
            "\n",
            "6Ô∏è‚É£  HYBRID SEARCH\n",
            "   ‚úì Semantic search: Understand intent & paraphrases\n",
            "   ‚úì Keyword search: Exact matches & technical terms\n",
            "   ‚úì Hybrid fusion: Combine for robustness\n",
            "   ‚Üí Weighted combination: (Œ± √ó semantic) + ((1-Œ±) √ó keyword)\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üéØ Key Insights:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "‚úÖ Chunking Impact\n",
            "   - Smaller chunks: More relevant but more overhead\n",
            "   - Overlap: Improves boundary handling\n",
            "   - Sentence-aware: Better semantic boundaries\n",
            "   ‚Üí Optimal: 400-600 chars with 50-100 char overlap\n",
            "\n",
            "‚úÖ Embedding Quality\n",
            "   - Model matters: nomic-embed-text (768) good baseline\n",
            "   - Speed: Typically 20-50ms per text\n",
            "   - Similarity: Works in high-dimensional space\n",
            "   ‚Üí Choose model based on latency SLA\n",
            "\n",
            "‚úÖ Reranking Works\n",
            "   - Single metric insufficient\n",
            "   - Multiple signals more robust\n",
            "   - Keyword catches what semantic misses\n",
            "   - Hybrid fusion: Best results\n",
            "\n",
            "‚úÖ RAG Pipeline\n",
            "   - Retrieval quality critical\n",
            "   - Reranking significantly improves results\n",
            "   - LLM quality depends on context\n",
            "   - Source attribution important\n",
            "   ‚Üí Iterative improvement: Measure and optimize\n",
            "\n",
            "‚úÖ Performance Trade-offs\n",
            "   - Speed vs quality: Always present\n",
            "   - Storage vs recall: Overlap improves recall\n",
            "   - Simplicity vs accuracy: Complex > simple\n",
            "   ‚Üí Profile your system: Identify bottlenecks\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üöÄ Next Steps to Explore:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "ADVANCED RETRIEVAL:\n",
            "  ‚ñ° Vector database indexing (HNSW, IVF)\n",
            "  ‚ñ° Approximate nearest neighbor search\n",
            "  ‚ñ° Quantization for faster search\n",
            "  ‚ñ° Distributed retrieval at scale\n",
            "\n",
            "RERANKING ENHANCEMENTS:\n",
            "  ‚ñ° Cross-encoder rerankers\n",
            "  ‚ñ° Learning-to-rank (LTR)\n",
            "  ‚ñ° Learned similarity functions\n",
            "  ‚ñ° Context-aware reranking\n",
            "\n",
            "RAG IMPROVEMENTS:\n",
            "  ‚ñ° Query decomposition\n",
            "  ‚ñ° Multi-turn conversation\n",
            "  ‚ñ° Fact verification\n",
            "  ‚ñ° Answer confidence scoring\n",
            "\n",
            "PRODUCTION SYSTEMS:\n",
            "  ‚ñ° Caching & memoization\n",
            "  ‚ñ° Rate limiting\n",
            "  ‚ñ° Error handling & fallbacks\n",
            "  ‚ñ° Monitoring & observability\n",
            "  ‚ñ° A/B testing different configs\n",
            "\n",
            "EVALUATION:\n",
            "  ‚ñ° NDCG@k, MRR, MAP metrics\n",
            "  ‚ñ° Human evaluation\n",
            "  ‚ñ° BLEU/ROUGE for generation\n",
            "  ‚ñ° Hallucination detection\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üí° Pro Tips:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "1. Start Simple, Iterate Fast\n",
            "   - Begin with fixed-size chunks, cosine similarity\n",
            "   - Measure performance baseline\n",
            "   - Add complexity only if needed\n",
            "\n",
            "2. Benchmark Everything\n",
            "   - Latency matters in production\n",
            "   - Quality metrics guide decisions\n",
            "   - Profile bottlenecks before optimizing\n",
            "\n",
            "3. Combine Multiple Approaches\n",
            "   - Semantic + keyword: More robust\n",
            "   - Multiple models: Ensemble effect\n",
            "   - Multiple rankers: Better ranking\n",
            "\n",
            "4. Test with Real Queries\n",
            "   - Lab benchmarks differ from real usage\n",
            "   - User queries reveal system weaknesses\n",
            "   - Iterate based on actual performance\n",
            "\n",
            "5. Monitor and Iterate\n",
            "   - Track metrics in production\n",
            "   - Log retrieval quality per query\n",
            "   - Continuously improve components\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üéì Learning Resources:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "Fundamental Papers:\n",
            "  - \"Attention is All You Need\" (Transformers)\n",
            "  - \"BERT: Pre-training...\" (Language models)\n",
            "  - \"Dense Passage Retrieval\" (Semantic search)\n",
            "  - \"ColBERT: Efficient...\" (Reranking)\n",
            "\n",
            "Tools to Explore:\n",
            "  - Faiss: Vector similarity search\n",
            "  - Pinecone: Managed vector DB\n",
            "  - Weaviate: Vector search platform\n",
            "  - Langchain: RAG frameworks\n",
            "\n",
            "Benchmarks:\n",
            "  - MS MARCO: Ranking benchmark\n",
            "  - Natural Questions: QA benchmark\n",
            "  - BEIR: IR evaluation suite\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "Your Next Challenge:\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "Build a complete RAG system with:\n",
            "  ‚úì Multi-document indexing\n",
            "  ‚úì Advanced chunking strategy\n",
            "  ‚úì Reranking with hybrid metrics\n",
            "  ‚úì LLM answer generation\n",
            "  ‚úì Performance monitoring\n",
            "  ‚úì Evaluation metrics\n",
            "\n",
            "Then benchmark against:\n",
            "  - Semantic search only\n",
            "  - Keyword search only\n",
            "  - Single-model embeddings\n",
            "\n",
            "And measure:\n",
            "  - Retrieval quality (NDCG, MRR)\n",
            "  - Answer quality (BLEU, human eval)\n",
            "  - Latency (ms per query)\n",
            "  - Memory usage (storage, inference)\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "Good luck! You now have the tools to build production-ready RAG systems! üöÄ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë           ADVANCED SEMANTIC SEARCH & RAG SUMMARY                     ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìö What We Covered:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1Ô∏è‚É£  ADVANCED CHUNKING\n",
        "   ‚úì Fixed-size chunking: Fast, predictable\n",
        "   ‚úì Sentence-aware: Better semantics, variable size\n",
        "   ‚úì Sliding window: More overlap, better recall\n",
        "   ‚Üí Trade-off: Granularity vs context vs storage\n",
        "\n",
        "2Ô∏è‚É£  MULTI-MODEL EMBEDDINGS\n",
        "   ‚úì Different models: Speed vs quality trade-off\n",
        "   ‚úì Embedding dimensions: Larger = better but slower\n",
        "   ‚úì Model selection: Based on latency & accuracy needs\n",
        "   ‚Üí Benchmark: Always measure latency for your use case\n",
        "\n",
        "3Ô∏è‚É£  RERANKING WITH MULTIPLE METRICS\n",
        "   ‚úì Cosine similarity: Angle-based, best for semantics\n",
        "   ‚úì Dot product: Raw alignment, magnitude-dependent\n",
        "   ‚úì Euclidean distance: Geometric distance\n",
        "   ‚úì BM25 keyword: Exact matches and frequency\n",
        "   ‚Üí Hybrid scoring: Combine multiple signals\n",
        "\n",
        "4Ô∏è‚É£  RETRIEVAL-AUGMENTED GENERATION (RAG)\n",
        "   ‚úì Retrieve: Find relevant chunks semantically\n",
        "   ‚úì Rerank: Score by multiple metrics\n",
        "   ‚úì Generate: Use LLM with retrieved context\n",
        "   ‚úì Cite: Include source attribution\n",
        "   ‚Üí Complete pipeline: Better answers with grounding\n",
        "\n",
        "5Ô∏è‚É£  PERFORMANCE BENCHMARKING\n",
        "   ‚úì Measure latency: Chunking, embedding, retrieval\n",
        "   ‚úì Track throughput: Texts/sec, chunks/sec\n",
        "   ‚úì Compare configurations: Find optimal trade-offs\n",
        "   ‚Üí Benchmark-driven: Data-informed decisions\n",
        "\n",
        "6Ô∏è‚É£  HYBRID SEARCH\n",
        "   ‚úì Semantic search: Understand intent & paraphrases\n",
        "   ‚úì Keyword search: Exact matches & technical terms\n",
        "   ‚úì Hybrid fusion: Combine for robustness\n",
        "   ‚Üí Weighted combination: (Œ± √ó semantic) + ((1-Œ±) √ó keyword)\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üéØ Key Insights:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "‚úÖ Chunking Impact\n",
        "   - Smaller chunks: More relevant but more overhead\n",
        "   - Overlap: Improves boundary handling\n",
        "   - Sentence-aware: Better semantic boundaries\n",
        "   ‚Üí Optimal: 400-600 chars with 50-100 char overlap\n",
        "\n",
        "‚úÖ Embedding Quality\n",
        "   - Model matters: nomic-embed-text (768) good baseline\n",
        "   - Speed: Typically 20-50ms per text\n",
        "   - Similarity: Works in high-dimensional space\n",
        "   ‚Üí Choose model based on latency SLA\n",
        "\n",
        "‚úÖ Reranking Works\n",
        "   - Single metric insufficient\n",
        "   - Multiple signals more robust\n",
        "   - Keyword catches what semantic misses\n",
        "   - Hybrid fusion: Best results\n",
        "\n",
        "‚úÖ RAG Pipeline\n",
        "   - Retrieval quality critical\n",
        "   - Reranking significantly improves results\n",
        "   - LLM quality depends on context\n",
        "   - Source attribution important\n",
        "   ‚Üí Iterative improvement: Measure and optimize\n",
        "\n",
        "‚úÖ Performance Trade-offs\n",
        "   - Speed vs quality: Always present\n",
        "   - Storage vs recall: Overlap improves recall\n",
        "   - Simplicity vs accuracy: Complex > simple\n",
        "   ‚Üí Profile your system: Identify bottlenecks\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üöÄ Next Steps to Explore:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "ADVANCED RETRIEVAL:\n",
        "  ‚ñ° Vector database indexing (HNSW, IVF)\n",
        "  ‚ñ° Approximate nearest neighbor search\n",
        "  ‚ñ° Quantization for faster search\n",
        "  ‚ñ° Distributed retrieval at scale\n",
        "\n",
        "RERANKING ENHANCEMENTS:\n",
        "  ‚ñ° Cross-encoder rerankers\n",
        "  ‚ñ° Learning-to-rank (LTR)\n",
        "  ‚ñ° Learned similarity functions\n",
        "  ‚ñ° Context-aware reranking\n",
        "\n",
        "RAG IMPROVEMENTS:\n",
        "  ‚ñ° Query decomposition\n",
        "  ‚ñ° Multi-turn conversation\n",
        "  ‚ñ° Fact verification\n",
        "  ‚ñ° Answer confidence scoring\n",
        "\n",
        "PRODUCTION SYSTEMS:\n",
        "  ‚ñ° Caching & memoization\n",
        "  ‚ñ° Rate limiting\n",
        "  ‚ñ° Error handling & fallbacks\n",
        "  ‚ñ° Monitoring & observability\n",
        "  ‚ñ° A/B testing different configs\n",
        "\n",
        "EVALUATION:\n",
        "  ‚ñ° NDCG@k, MRR, MAP metrics\n",
        "  ‚ñ° Human evaluation\n",
        "  ‚ñ° BLEU/ROUGE for generation\n",
        "  ‚ñ° Hallucination detection\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üí° Pro Tips:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "1. Start Simple, Iterate Fast\n",
        "   - Begin with fixed-size chunks, cosine similarity\n",
        "   - Measure performance baseline\n",
        "   - Add complexity only if needed\n",
        "\n",
        "2. Benchmark Everything\n",
        "   - Latency matters in production\n",
        "   - Quality metrics guide decisions\n",
        "   - Profile bottlenecks before optimizing\n",
        "\n",
        "3. Combine Multiple Approaches\n",
        "   - Semantic + keyword: More robust\n",
        "   - Multiple models: Ensemble effect\n",
        "   - Multiple rankers: Better ranking\n",
        "\n",
        "4. Test with Real Queries\n",
        "   - Lab benchmarks differ from real usage\n",
        "   - User queries reveal system weaknesses\n",
        "   - Iterate based on actual performance\n",
        "\n",
        "5. Monitor and Iterate\n",
        "   - Track metrics in production\n",
        "   - Log retrieval quality per query\n",
        "   - Continuously improve components\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üéì Learning Resources:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Fundamental Papers:\n",
        "  - \"Attention is All You Need\" (Transformers)\n",
        "  - \"BERT: Pre-training...\" (Language models)\n",
        "  - \"Dense Passage Retrieval\" (Semantic search)\n",
        "  - \"ColBERT: Efficient...\" (Reranking)\n",
        "\n",
        "Tools to Explore:\n",
        "  - Faiss: Vector similarity search\n",
        "  - Pinecone: Managed vector DB\n",
        "  - Weaviate: Vector search platform\n",
        "  - Langchain: RAG frameworks\n",
        "\n",
        "Benchmarks:\n",
        "  - MS MARCO: Ranking benchmark\n",
        "  - Natural Questions: QA benchmark\n",
        "  - BEIR: IR evaluation suite\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Your Next Challenge:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Build a complete RAG system with:\n",
        "  ‚úì Multi-document indexing\n",
        "  ‚úì Advanced chunking strategy\n",
        "  ‚úì Reranking with hybrid metrics\n",
        "  ‚úì LLM answer generation\n",
        "  ‚úì Performance monitoring\n",
        "  ‚úì Evaluation metrics\n",
        "\n",
        "Then benchmark against:\n",
        "  - Semantic search only\n",
        "  - Keyword search only\n",
        "  - Single-model embeddings\n",
        "\n",
        "And measure:\n",
        "  - Retrieval quality (NDCG, MRR)\n",
        "  - Answer quality (BLEU, human eval)\n",
        "  - Latency (ms per query)\n",
        "  - Memory usage (storage, inference)\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "Good luck! You now have the tools to build production-ready RAG systems! üöÄ\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ed841d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.9.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
