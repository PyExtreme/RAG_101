{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete RAG System with Benchmarking & Evaluation\n",
    "\n",
    "This notebook implements a production-ready RAG system with comprehensive benchmarking against baseline approaches.\n",
    "\n",
    "## Challenge Overview\n",
    "\n",
    "Build a complete RAG system with:\n",
    "- Multi-document indexing\n",
    "- Advanced chunking strategy\n",
    "- Reranking with hybrid metrics\n",
    "- LLM answer generation\n",
    "- Performance monitoring\n",
    "- Evaluation metrics\n",
    "\n",
    "Then benchmark against:\n",
    "- Semantic search only\n",
    "- Keyword search only\n",
    "- Single-model embeddings\n",
    "\n",
    "And measure:\n",
    "- Retrieval quality (NDCG, MRR)\n",
    "- Answer quality (BLEU, human eval)\n",
    "- Latency (ms per query)\n",
    "- Memory usage (storage, inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: Setup and Dependencies\n\n### What we're doing:\nSetting up the Python environment with all required libraries for our RAG system.\n\n### Why it matters:\n- **ChromaDB**: Vector database for storing embeddings\n- **Ollama**: Local LLM and embedding model interface\n- **NumPy/Pandas**: Data manipulation and numerical operations\n- **rank-bm25**: Traditional keyword-based search (BM25 algorithm)\n- **psutil**: Monitor system performance (memory, CPU)\n- **seaborn/matplotlib**: Create beautiful visualizations\n\n### Learning tip:\nIn production, you'd use a requirements.txt file, but for educational notebooks, installing packages inline makes it more portable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 1: Install Dependencies\n# ============================================\n# This cell installs all Python packages needed for the RAG system.\n# Run this once at the start of your session.\n\nimport subprocess\nimport sys\n\npackages = [\n    'chromadb>=0.5.0',  # Vector database\n    'ollama',            # Ollama API client\n    'numpy',             # Numerical computing\n    'matplotlib',        # Plotting\n    'scikit-learn',      # Machine learning utilities\n    'pandas',            # Data manipulation\n    'rank-bm25',         # BM25 keyword search\n    'requests',          # HTTP requests\n    'nltk',              # Natural language toolkit\n    'psutil',            # System monitoring\n    'seaborn',           # Statistical visualization\n]\n\nprint(\"Installing packages...\")\nfor package in packages:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n\nprint(\"\u2705 All packages installed!\")\nprint(\"\\n\ud83d\udcdd Prerequisites:\")\nprint(\"- Make sure Ollama is running: ollama serve\")\nprint(\"- Pull embedding model: ollama pull nomic-embed-text\")\nprint(\"- Optional for generation: ollama pull mistral\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 2: Import Libraries and Configure Environment\n# ============================================\n# Import all necessary libraries and configure the environment.\n# This sets up our workspace with all the tools we need.\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport time\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\nimport psutil\nimport os\n\n# For text processing and evaluation\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.metrics import ndcg_score\nimport nltk\n\n# For vector storage (optional, not used in this version)\nimport chromadb\nfrom chromadb.config import Settings\n\n# Download NLTK resources for text processing\ntry:\n    nltk.download('punkt', quiet=True)\n    nltk.download('stopwords', quiet=True)\nexcept:\n    pass\n\n# Configure visualization style for better-looking plots\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"\u2705 All imports successful!\")\nprint(\"\\n\ud83d\udcca Ready for RAG benchmarking\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: Sample Document Corpus\n\n### What we're doing:\nCreating a diverse knowledge base of 6 documents covering ML, NLP, RAG, embeddings, and evaluation topics.\n\n### Why it matters:\n- **Diverse topics**: Tests system's ability to handle different content types\n- **Ground truth**: Each test query has known relevant documents for evaluation\n- **Real-world simulation**: Documents mimic technical documentation and educational content\n\n### Key concepts:\n- **Documents**: Our knowledge base to search through\n- **Test queries**: Questions with known correct answers (ground truth)\n- **Relevant docs**: Which documents should be retrieved for each query\n- **Reference answers**: Expected answers for evaluating generation quality"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 3: Define Document Corpus and Test Queries\n# ============================================\n# Create our knowledge base (6 documents) and test queries with ground truth.\n# This simulates a real-world scenario where we have documentation to search.\n\n# Sample documents covering different topics\nDOCUMENTS = {\n    \"doc1_ml_basics\": \"\"\"\nMachine Learning: Fundamentals and Applications\n\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve from experience \nwithout being explicitly programmed. At its core, machine learning involves creating algorithms that can discover \npatterns in data and make predictions or decisions based on those patterns.\n\nThe three main types of machine learning are supervised learning, unsupervised learning, and reinforcement learning. \nSupervised learning uses labeled data to train models that can predict outputs for new inputs. Common applications \ninclude image classification, spam detection, and sentiment analysis. Unsupervised learning works with unlabeled \ndata to discover hidden patterns or structures. This includes clustering, dimensionality reduction, and anomaly \ndetection. Reinforcement learning trains agents to make sequential decisions through trial and error, receiving \nrewards or penalties for actions.\n\nNeural networks are a key component of modern machine learning. They consist of layers of interconnected nodes \nthat process information similarly to neurons in the human brain. Deep learning uses neural networks with many \nlayers to learn hierarchical representations of data. This approach has achieved breakthrough results in computer \nvision, natural language processing, and speech recognition.\n\"\"\",\n    \n    \"doc2_neural_networks\": \"\"\"\nNeural Networks and Deep Learning\n\nNeural networks are computational models inspired by the structure and function of biological neural networks in \nanimal brains. An artificial neural network consists of layers of interconnected nodes called neurons. Each connection \nhas a weight that adjusts as learning proceeds, allowing the network to learn complex patterns.\n\nA basic neural network has three types of layers: an input layer that receives data, hidden layers that process \ninformation, and an output layer that produces results. The training process involves forward propagation, where \ndata flows through the network to generate predictions, and backpropagation, where errors are propagated backward \nto adjust weights and minimize loss.\n\nDeep learning refers to neural networks with multiple hidden layers. These deep architectures can learn hierarchical \nfeatures from raw data. Convolutional Neural Networks (CNNs) excel at image processing by using convolutional layers \nto detect spatial patterns. Recurrent Neural Networks (RNNs) and their variants like LSTMs are designed for sequential \ndata, maintaining memory of previous inputs. Transformers, introduced in 2017, use attention mechanisms to process \nsequences in parallel and have become dominant in natural language processing.\n\"\"\",\n    \n    \"doc3_nlp\": \"\"\"\nNatural Language Processing: Modern Techniques\n\nNatural Language Processing (NLP) is a field at the intersection of linguistics, computer science, and artificial \nintelligence that focuses on enabling computers to understand, interpret, and generate human language. Modern NLP \nhas been revolutionized by deep learning approaches, particularly transformer-based models.\n\nWord embeddings are a fundamental concept in NLP, representing words as dense vectors in a continuous space where \nsemantic relationships are captured by geometric relationships. Word2Vec and GloVe were early embedding methods, \nbut contextual embeddings from models like BERT and GPT provide dynamic representations that vary based on context.\n\nThe transformer architecture, introduced in the paper 'Attention Is All You Need', uses self-attention mechanisms \nto process sequences in parallel rather than sequentially. This enables efficient training on large datasets and \ncapture of long-range dependencies. BERT (Bidirectional Encoder Representations from Transformers) uses masked \nlanguage modeling for pre-training, while GPT (Generative Pre-trained Transformer) uses autoregressive language \nmodeling. These pre-trained models can be fine-tuned for various downstream tasks like question answering, text \nclassification, and named entity recognition.\n\nRecent advances include instruction-tuned models that follow human instructions more naturally, and retrieval-augmented \ngeneration (RAG) systems that combine language models with information retrieval to ground responses in factual knowledge.\n\"\"\",\n    \n    \"doc4_rag_systems\": \"\"\"\nRetrieval-Augmented Generation Systems\n\nRetrieval-Augmented Generation (RAG) is an approach that combines the strengths of large language models with \ninformation retrieval systems. Instead of relying solely on knowledge encoded in model parameters during training, \nRAG systems retrieve relevant documents from an external knowledge base and use them as context for generation.\n\nA typical RAG pipeline has three main stages: retrieval, reranking, and generation. During retrieval, relevant \ndocuments are identified using semantic search with dense embeddings or traditional keyword-based methods like BM25. \nThe reranking stage scores and reorders retrieved documents to prioritize the most relevant ones. Finally, the \ngeneration stage uses a language model to produce an answer conditioned on the query and retrieved context.\n\nDense retrieval methods encode queries and documents into embedding vectors using neural networks. Similarity is \ncomputed using metrics like cosine similarity or dot product. This captures semantic similarity beyond exact keyword \nmatches. Hybrid approaches combine dense and sparse retrieval for better performance. Vector databases like Pinecone, \nWeaviate, and ChromaDB provide efficient storage and retrieval of embeddings at scale.\n\nKey challenges in RAG include handling multi-hop reasoning across multiple documents, ensuring factual consistency, \nand dealing with outdated or conflicting information. Advanced techniques include query decomposition, iterative \nretrieval, and answer verification to improve reliability.\n\"\"\",\n    \n    \"doc5_embeddings\": \"\"\"\nVector Embeddings and Semantic Search\n\nVector embeddings are numerical representations of data that capture semantic meaning in a continuous vector space. \nIn this space, similar items are positioned close together while dissimilar items are far apart. This property enables \nsemantic search, where queries and documents are compared based on meaning rather than exact keyword matches.\n\nCreating effective embeddings requires training on large amounts of data. Models learn to map input text to vectors \nsuch that semantically similar texts have similar embeddings. Modern embedding models like sentence-transformers are \nbased on transformer architectures and trained using contrastive learning objectives. These models produce high-quality \nembeddings that work well across diverse domains.\n\nSimilarity between embeddings is typically measured using cosine similarity, which computes the angle between vectors, \nor Euclidean distance, which measures geometric distance. Dot product is another common metric that considers both \nangle and magnitude. The choice of metric depends on whether embeddings are normalized and the specific use case.\n\nApproximate nearest neighbor (ANN) algorithms enable efficient similarity search in large embedding collections. \nMethods like HNSW (Hierarchical Navigable Small World) and IVF (Inverted File Index) trade off some accuracy for \nsignificant speed improvements. This makes real-time semantic search feasible even with millions of documents.\n\"\"\",\n    \n    \"doc6_evaluation\": \"\"\"\nEvaluating Information Retrieval and RAG Systems\n\nProper evaluation is critical for developing effective retrieval and RAG systems. Metrics must capture both the \nquality of retrieved documents and the quality of generated answers.\n\nRetrieval metrics assess how well relevant documents are retrieved and ranked. Precision measures the fraction of \nretrieved documents that are relevant, while recall measures the fraction of relevant documents that are retrieved. \nMean Average Precision (MAP) averages precision across different recall levels. Normalized Discounted Cumulative Gain \n(NDCG) accounts for both relevance and ranking position, giving more weight to highly relevant documents appearing \nearly in results. Mean Reciprocal Rank (MRR) measures the average position of the first relevant document.\n\nFor generation quality, BLEU (Bilingual Evaluation Understudy) score measures n-gram overlap between generated and \nreference answers. ROUGE focuses on recall of n-grams and is commonly used for summarization. METEOR considers \nsynonyms and paraphrases for more flexible matching. However, these automated metrics have limitations and don't \nalways correlate well with human judgments.\n\nHuman evaluation remains the gold standard. Annotators can assess relevance, factual accuracy, completeness, and \nfluency. However, human evaluation is expensive and time-consuming. Modern approaches use LLM-based evaluation where \none language model judges outputs from another, showing promising correlation with human assessments.\n\nEnd-to-end evaluation should measure latency, throughput, memory usage, and cost in addition to quality metrics. \nA/B testing with real users provides the ultimate validation of system improvements.\n\"\"\",\n}\n\n# Test queries with ground truth - this is our evaluation dataset\n# Each query has: the question, which docs are relevant, and a reference answer\nTEST_QUERIES = [\n    {\n        \"query\": \"How does machine learning work?\",\n        \"relevant_docs\": [\"doc1_ml_basics\", \"doc2_neural_networks\"],\n        \"reference_answer\": \"Machine learning works by creating algorithms that discover patterns in data and make predictions based on those patterns, rather than being explicitly programmed.\"\n    },\n    {\n        \"query\": \"What are neural networks and how are they structured?\",\n        \"relevant_docs\": [\"doc2_neural_networks\", \"doc1_ml_basics\"],\n        \"reference_answer\": \"Neural networks are computational models inspired by biological brains, consisting of layers of interconnected nodes. They have input layers, hidden layers that process information, and output layers that produce results.\"\n    },\n    {\n        \"query\": \"Explain transformers in natural language processing\",\n        \"relevant_docs\": [\"doc3_nlp\", \"doc2_neural_networks\"],\n        \"reference_answer\": \"Transformers are neural network architectures that use self-attention mechanisms to process sequences in parallel. They enable efficient training and capture of long-range dependencies, and have become dominant in NLP.\"\n    },\n    {\n        \"query\": \"What is RAG and how does it work?\",\n        \"relevant_docs\": [\"doc4_rag_systems\", \"doc3_nlp\"],\n        \"reference_answer\": \"Retrieval-Augmented Generation (RAG) combines language models with information retrieval. It retrieves relevant documents from a knowledge base and uses them as context for generating answers, rather than relying only on model parameters.\"\n    },\n    {\n        \"query\": \"How do you measure similarity between embeddings?\",\n        \"relevant_docs\": [\"doc5_embeddings\", \"doc4_rag_systems\"],\n        \"reference_answer\": \"Similarity between embeddings is typically measured using cosine similarity (angle between vectors), Euclidean distance (geometric distance), or dot product (considering both angle and magnitude).\"\n    },\n    {\n        \"query\": \"What metrics are used to evaluate retrieval systems?\",\n        \"relevant_docs\": [\"doc6_evaluation\"],\n        \"reference_answer\": \"Retrieval systems are evaluated using metrics like NDCG (ranking quality with relevance), MRR (position of first relevant document), MAP (average precision), and precision/recall (retrieval quality).\"\n    },\n]\n\nprint(f\"\u2705 Loaded {len(DOCUMENTS)} documents\")\nprint(f\"\u2705 Loaded {len(TEST_QUERIES)} test queries\")\nprint(f\"\\n\ud83d\udcca Document Statistics:\")\nfor doc_id, content in DOCUMENTS.items():\n    print(f\"  {doc_id}: {len(content)} chars, {len(content.split())} words\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 3: Core Components\n\n### What we're doing:\nBuilding the fundamental building blocks of our RAG system: data structures and monitoring tools.\n\n### Components we'll create:\n1. **TextChunk**: Represents a piece of text with metadata (source, ID, strategy used)\n2. **PerformanceMetrics**: Records timing and memory usage for each operation\n3. **PerformanceMonitor**: Tracks all metrics and generates statistics\n\n### Why monitoring matters:\nIn production systems, you need to know:\n- Which operations are slow (bottlenecks)\n- How much memory you're using (cost optimization)\n- P95/P99 latencies (user experience)\n- Performance trends over time\n\n### Learning tip:\nAlways instrument your code from the start. It's much harder to add monitoring later!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 4: Core Data Structures and Performance Monitoring\n\n**What this code does:**\n- Defines `TextChunk` class to represent pieces of text with metadata\n- Creates `PerformanceMetrics` to track timing and memory for each operation  \n- Implements `PerformanceMonitor` to collect and analyze all performance data\n\n**Why it matters:**\n- **Structured data**: TextChunk keeps text organized with source info and embeddings\n- **Performance tracking**: Essential for finding bottlenecks in production\n- **P95/P99 metrics**: Shows tail latency (what 95%/99% of users experience)\n\n**Key concepts:**\n- `@dataclass`: Python decorator for creating classes with less boilerplate\n- Memory monitoring: Uses `psutil` to track RAM usage\n- Percentiles: P95 means 95% of requests are faster than this value\n\n**Production insight:**\nAlways instrument your code from day one! It's much harder to add monitoring later when you're debugging performance issues at 2am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 5: Ollama Embedder with Caching\n\n**What this code does:**\n- Creates a class to generate embeddings using Ollama's API\n- Implements caching to avoid re-computing embeddings for same text\n- Tracks performance metrics for every embedding operation\n\n**Why it matters:**\n- **Embeddings are expensive**: 20-50ms per text adds up quickly!\n- **Caching = 99% cost reduction**: Same text = instant cache hit\n- **Performance tracking**: Helps identify if embeddings are your bottleneck\n\n**How embeddings work:**\n1. Send text to Ollama API\n2. Model converts text to 768-dimensional vector\n3. Similar texts get similar vectors\n4. Cache the result for future use\n\n**Real-world numbers:**\n- Without cache: 1000 texts = 30+ seconds\n- With cache: Same 1000 texts second time = instant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 6: Advanced Sentence-Aware Chunker\n\n**What this code does:**\n- Splits documents into chunks while respecting sentence boundaries\n- Implements overlapping chunks to preserve context across boundaries\n- Tracks how many sentences are in each chunk\n\n**Why sentence-aware chunking:**\n- **Better semantics**: Don't cut sentences in half\n- **Overlap helps**: Captures information that spans chunk boundaries\n- **Flexible size**: Adjusts to sentence length naturally\n\n**The chunking algorithm:**\n1. Split document into sentences\n2. Group sentences until reaching chunk_size (512 chars)\n3. Keep overlap (128 chars) from previous chunk\n4. Repeat until document is fully chunked\n\n**Trade-offs:**\n- Larger chunks: More context but less precise retrieval\n- Smaller chunks: More precise but may miss context\n- Overlap: Better recall but uses more storage (worth it!)\n\n**Our settings (512/128):**\nGood balance for technical documentation and Q&A systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Index Documents\n\n### What we're doing:\nProcessing all 6 documents through our pipeline to prepare for retrieval.\n\n### The indexing pipeline:\n1. **Chunking**: Break documents into manageable pieces\n2. **Embedding**: Convert text chunks to numerical vectors  \n3. **BM25 index**: Build keyword search index\n\n### Why indexing matters:\n- Happens once offline, speeds up all future queries\n- Pre-computing embeddings saves 95% of query latency\n- Multiple indexes (semantic + keyword) enable hybrid search\n\n### What you'll see:\n- How many chunks per document\n- Embedding generation speed (embeddings/second)\n- Total memory and storage used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 7: Chunk All Documents\n\n**What this code does:**\n- Applies our sentence-aware chunker to all 6 documents\n- Stores chunks in both a flat list and per-document map\n- Reports chunking statistics\n\n**What to observe:**\n- Documents with more text = more chunks\n- Average chunk size should be ~400-500 characters\n- Total chunks determines retrieval search space\n\n**The chunk distribution:**\n- Each document: 3-5 chunks typically\n- Total corpus: ~25-35 chunks (our test set)\n- Production: Could be 10,000s or millions of chunks\n\n**Performance note:**\nChunking is fast (<1ms per document). The bottleneck comes next: embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 8: Generate and Store Embeddings\n\n**What this code does:**\n- Generates embeddings for all ~30 chunks\n- Measures throughput (embeddings per second)\n- Stores embeddings in chunk metadata for later retrieval\n\n**This is the expensive step!**\n- Embedding generation: Dominant cost in RAG systems\n- Our system: ~20-50ms per embedding (depends on model)\n- Total time: 30 chunks \u00d7 30ms = ~1 second\n\n**Why embeddings are cached:**\n- Generate once, use forever (until document changes)\n- Query embeddings: Generated on-the-fly (1 per query)\n- Chunk embeddings: Pre-computed (thousands cached)\n\n**Optimization opportunities:**\n- Use smaller models for speed (sacrifice quality)\n- Batch embeddings (some models faster in batches)\n- Quantize vectors to reduce memory (int8 vs float32)\n\n**Watch the throughput number:**\nHigher = better! Aim for >20 embeddings/sec minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 9: Create BM25 Keyword Index\n\n**What this code does:**\n- Tokenizes all chunks (splits into words)\n- Builds BM25 index for keyword-based retrieval\n- Super fast: keyword indexing takes milliseconds!\n\n**What is BM25?**\n- Best Match 25: Classic information retrieval algorithm\n- Scores documents based on term frequency and document length\n- Used by search engines before neural methods\n\n**Why we still use BM25 in 2024:**\n- Extremely fast (~1ms for retrieval)\n- Great for exact keyword matches\n- Complements semantic search perfectly\n\n**BM25 vs Semantic:**\n- BM25: \"neural network\" matches \"neural network\" exactly\n- Semantic: \"neural network\" also matches \"deep learning model\"\n- Hybrid: Gets both! Best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Retrieval Strategies\n\n### What we're building:\nThree different retrieval systems to compare:\n1. **Semantic only**: Uses embeddings and cosine similarity\n2. **Keyword only**: Uses BM25 term matching\n3. **Hybrid**: Combines both with weighted scoring\n\n### Why compare multiple strategies:\n- Different queries need different approaches\n- \"What is machine learning?\" \u2192 semantic wins\n- \"BM25 algorithm explained\" \u2192 keyword wins  \n- Most queries \u2192 hybrid wins!\n\n### The hybrid formula:\n`score = \u03b1 \u00d7 semantic_score + (1-\u03b1) \u00d7 keyword_score`\n\nWhere \u03b1=0.6 means 60% semantic, 40% keyword\n\n### You'll learn:\n- How each retrieval method works internally\n- Speed vs quality trade-offs\n- When to use which strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 10: Implement Three Retrieval Strategies\n\n**What this code does:**\nImplements three complete retrieval systems that find relevant chunks for queries.\n\n**1. SemanticRetriever:**\n- Embeds the query\n- Calculates cosine similarity with all chunk embeddings\n- Returns top-k most similar chunks\n- **Best for**: Natural language queries, paraphrasing\n\n**2. KeywordRetriever:**\n- Tokenizes the query\n- Uses BM25 to score chunks by term overlap\n- Returns top-k highest scoring chunks\n- **Best for**: Exact matches, technical terms\n\n**3. HybridRetriever:**\n- Runs both semantic and keyword retrieval\n- Normalizes scores to [0, 1] range\n- Combines with weighted sum (\u03b1=0.6 for semantic)\n- Returns top-k by combined score\n- **Best for**: Production use! Most robust.\n\n**The math - Cosine Similarity:**\n```\nsimilarity = dot(query_vec, doc_vec) / (||query_vec|| \u00d7 ||doc_vec||)\n```\nResult is between -1 and 1 (usually 0 to 1 for text)\n\n**Performance:**\n- Semantic: ~10-20ms (embedding + similarity calculation)\n- Keyword: ~1-3ms (just BM25 scoring)\n- Hybrid: ~20-30ms (runs both + combining)\n\n**All retrievers auto-track performance via our monitor!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation Metrics\n\n### What we're implementing:\nComprehensive metrics to measure retrieval and generation quality.\n\n### Retrieval Metrics:\n- **NDCG@k** (Normalized Discounted Cumulative Gain): Rewards relevant docs appearing early\n- **MRR** (Mean Reciprocal Rank): Position of first relevant document\n- **Precision@k**: What fraction of top-k are relevant?\n- **Recall@k**: What fraction of relevant docs are in top-k?\n\n### Generation Metrics:\n- **BLEU**: N-gram overlap between generated and reference answers\n- **Token Overlap**: Simple word overlap ratio\n\n### Why multiple metrics:\n- No single metric tells the whole story\n- NDCG: Best overall retrieval quality metric\n- MRR: User experience (did we find something useful fast?)\n- Precision: Are results accurate?\n- Recall: Are we missing relevant docs?\n\n### The evaluation dataset:\n- 6 test queries with ground truth\n- Known relevant documents for each query\n- Reference answers for generation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 11: Evaluation Metric Implementations\n\n**What this code does:**\nImplements standard IR (Information Retrieval) and NLG (Natural Language Generation) metrics.\n\n**RetrievalEvaluator class:**\n\n**1. NDCG@k (Normalized Discounted Cumulative Gain):**\n- Most important retrieval metric!\n- Formula: DCG / IDCG where DCG = \u03a3(relevance / log2(position + 1))\n- Rewards: Relevant docs at top positions\n- Score: 0 to 1 (1 = perfect ranking)\n- Used by: Google, Bing, academic papers\n\n**2. MRR (Mean Reciprocal Rank):**\n- Formula: 1 / position_of_first_relevant_doc\n- Example: First relevant at position 3 \u2192 MRR = 0.333\n- Measures: How quickly users find what they need\n- Used by: Question answering systems\n\n**3. Precision and Recall:**\n- Precision = relevant_retrieved / total_retrieved\n- Recall = relevant_retrieved / total_relevant\n- Trade-off: Can't maximize both simultaneously!\n\n**GenerationEvaluator class:**\n\n**1. BLEU Score:**\n- Measures n-gram overlap (1-grams, 2-grams, etc.)\n- Originally for machine translation\n- Range: 0 to 1 (higher = better overlap)\n- Limitation: Doesn't capture semantic similarity\n\n**2. Token Overlap:**\n- Simpler metric: shared words / total unique words\n- Quick approximation of answer quality\n- Better than nothing when no reference answers!\n\n**Production tip:**\nAlways use multiple metrics! NDCG alone doesn't tell you about recall. BLEU alone doesn't measure factual correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Complete RAG System\n\n### What we're building:\nThe full Retrieval-Augmented Generation pipeline that combines:\n1. **Retrieval**: Find relevant chunks (covered above)\n2. **Context building**: Format chunks for LLM\n3. **Generation**: Use LLM to generate answer\n4. **Source attribution**: Track where answers come from\n\n### The RAG pipeline:\n```\nQuery \u2192 Retrieve chunks \u2192 Build context \u2192 LLM generates \u2192 Answer\n```\n\n### Why RAG beats pure LLM:\n- **Factual grounding**: Answers based on retrieved docs\n- **Up-to-date**: No retraining needed for new info\n- **Attributable**: Can cite sources\n- **Reduced hallucination**: Context constrains generation\n\n### The prompt template:\nWe give the LLM:\n- Retrieved context (top 3 chunks)\n- The user's question  \n- Instruction to answer concisely\n\n### You'll create:\n3 complete RAG systems (semantic, keyword, hybrid) for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 12: Complete RAG System Implementation\n\n**What this code does:**\nImplements a full RAG system that retrieves context and generates answers using an LLM.\n\n**The RAGSystem class has two key methods:**\n\n**1. generate_answer():**\n- Takes query + retrieved chunks\n- Builds formatted context (top 3 chunks, truncated to 400 chars each)\n- Creates prompt with context + question\n- Calls Ollama API to generate answer\n- Handles timeouts and errors gracefully\n- Returns answer + latency\n\n**The prompt structure:**\n```\nContext: [Source 1]...[Source 2]...[Source 3]...\nQuestion: {user_question}\nAnswer:\n```\n\n**2. answer() - The complete pipeline:**\n- Retrieves top-k relevant chunks\n- Optionally generates answer using LLM\n- Tracks total latency\n- Returns everything: chunks, scores, answer, timing\n\n**LLM settings:**\n- Temperature: 0.1 (low = more deterministic, less creative)\n- Max tokens: 150 (keep answers concise)\n- Model: mistral (good balance of speed and quality)\n\n**Error handling:**\n- Timeout after 60s \u2192 return retrieval-only\n- LLM unavailable \u2192 return context without generation\n- Always fails gracefully!\n\n**We create 3 RAG systems:**\n- rag_semantic: Uses only semantic retrieval\n- rag_keyword: Uses only BM25 retrieval\n- rag_hybrid: Uses combined retrieval (best!)\n\n**Performance expectations:**\n- Retrieval: 10-30ms\n- Generation: 2-5 seconds (dominant cost!)\n- Total: 2-5 seconds per query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Run Comprehensive Benchmark\n\n### What we're doing:\nTesting all 3 RAG systems on 6 diverse queries to compare their performance.\n\n### The benchmark process:\n1. For each system (semantic, keyword, hybrid):\n2. For each test query:\n   - Run retrieval + generation\n   - Evaluate retrieval quality (NDCG, MRR, P/R)\n   - Evaluate generation quality (BLEU if generated)\n   - Measure latency\n3. Calculate aggregate statistics\n\n### What we're measuring:\n- **Quality**: Which system retrieves better? Generates better answers?\n- **Speed**: What's the latency per system?\n- **Consistency**: Low variance = reliable performance\n\n### The output:\n- Per-query metrics for all systems\n- Average metrics per system\n- Detailed results in a pandas DataFrame\n\n### Expected results:\n- Hybrid should win on retrieval quality\n- Keyword should be fastest\n- Semantic handles paraphrasing best\n\n### Note on generation:\nSet `generate_answers=False` if you don't have Ollama mistral model installed. Benchmark will still measure retrieval quality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 13: Run the Benchmark (Main Experiment)\n\n**What this code does:**\nRuns a complete benchmark comparing all three RAG systems on 6 test queries.\n\n**The benchmark flow:**\n```\nFor each system (Semantic, Keyword, Hybrid):\n  For each of 6 test queries:\n    1. Run RAG pipeline (retrieve + generate)\n    2. Evaluate retrieval:\n       - Calculate NDCG@5\n       - Calculate MRR\n       - Calculate Precision@5 and Recall@5\n    3. Evaluate generation (if enabled):\n       - Calculate BLEU score\n       - Calculate token overlap\n    4. Record latency\n  Calculate and print averages for system\nReturn all results in DataFrame\n```\n\n**What you'll see printed:**\n- Progress for each query\n- Metrics for each query\u00d7system combination\n- System averages at the end\n- Final DataFrame with all results\n\n**Reading the results:**\n- NDCG@5 close to 1.0 = excellent retrieval\n- MRR = 1.0 = relevant doc in position 1 (perfect!)\n- Precision@5 = 0.6 = 3/5 retrieved docs are relevant\n- Lower latency = better user experience\n\n**The big question this answers:**\n\"Should I use semantic, keyword, or hybrid search in production?\"\n\n**Typical outcome:**\n- Hybrid wins on quality (NDCG, MRR)\n- Keyword wins on speed\n- Semantic good for natural queries\n- **Conclusion: Use hybrid for production!**\n\n**The results are stored in `benchmark_df` for analysis and visualization in next sections.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Results Analysis and Visualization\n\n### What we're doing:\nAnalyzing benchmark results and creating visualizations to understand system performance.\n\n### The analysis includes:\n1. **Summary statistics**: Mean, std dev, min, max for all metrics\n2. **Best system per metric**: Which wins on NDCG? MRR? Latency?\n3. **Bar charts**: Compare metrics across systems with error bars\n4. **Latency analysis**: Average and distribution of response times\n5. **Radar chart**: Multi-dimensional comparison view\n\n### Why visualization matters:\n- Numbers alone don't tell the story\n- Visualizations reveal patterns and trade-offs\n- Easy to share with stakeholders\n- Helps make architectural decisions\n\n### What to look for:\n- Clear winner on quality? (Probably hybrid)\n- Speed vs quality trade-off visible?\n- High variance = inconsistent performance (bad!)\n- All systems good on some metrics? (Shows diversity of query types)\n\n### These visualizations answer:\n- \"Which system should we use in production?\"\n- \"Is the quality improvement worth the latency cost?\"\n- \"Do all query types favor the same approach?\"\n\n### You'll create:\n- 4-panel retrieval quality comparison\n- Latency comparison (bar + box plots)\n- Radar chart for holistic view\n- Performance monitoring summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 14: Statistical Summary and Best Systems\n\n**What this code does:**\n- Aggregates results across all queries per system\n- Calculates mean, std dev, and latency percentiles\n- Identifies best system for each metric\n\n**Reading the summary table:**\n- Each row = one system (Semantic, Keyword, Hybrid)\n- Columns show mean and std for each metric\n- Lower std = more consistent performance\n\n**The \"best systems\" section:**\n- Shows winner for NDCG, MRR, Precision, Recall\n- Best system typically = Hybrid (combines strengths)\n- Look at the margins: close scores = little difference\n\n**What the statistics tell you:**\n- **High mean, low std**: Consistently good (ideal!)\n- **High mean, high std**: Sometimes great, sometimes poor (risky)\n- **Low mean, low std**: Consistently mediocre (not useful)\n\n**Production decision-making:**\nIf hybrid wins by small margin (<0.05), consider if complexity is worth it.\nIf hybrid wins by large margin (>0.10), definitely use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 15: Retrieval Quality Visualization (4-Panel)\n\n**What this code does:**\nCreates a 2\u00d72 grid comparing all systems on 4 key retrieval metrics.\n\n**The four panels:**\n1. **NDCG@5**: Best overall retrieval quality metric\n2. **MRR**: How quickly do users find relevant results?\n3. **Precision@5**: What fraction of results are relevant?\n4. **Recall@5**: What fraction of relevant docs do we find?\n\n**How to read the charts:**\n- Taller bars = better performance\n- Error bars show variance (consistency)\n- Numbers on top show exact values\n- All metrics normalized to 0-1 scale\n\n**What to look for:**\n- Does one system dominate all metrics? (Rare!)\n- Trade-offs visible? (E.g., high precision, low recall)\n- Small differences (<0.05) may not matter in practice\n\n**Typical pattern:**\n- Semantic: High on natural language queries\n- Keyword: High on exact term matches\n- Hybrid: Best overall (balances both)\n\n**The visualization makes it easy to:**\n- Compare at a glance\n- Spot trends and patterns\n- Present findings to stakeholders\n- Make architecture decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 16: Latency Analysis (Bar + Box Plots)\n\n**What this code does:**\nVisualizes response time distribution for all three systems.\n\n**The two plots:**\n\n**1. Average Latency (Bar chart):**\n- Shows mean latency per system\n- Error bars = standard deviation\n- Helps compare typical performance\n\n**2. Latency Distribution (Box plot):**\n- Shows full distribution (min, Q1, median, Q3, max)\n- Reveals outliers and consistency\n- More informative than just mean!\n\n**Why latency matters:**\n- User experience: >500ms feels slow\n- Cost: Slower = fewer queries per second = more infrastructure\n- SLA compliance: P95 and P99 are what you promise customers\n\n**What to look for:**\n- Median vs mean: Big difference = outliers exist\n- Box size: Large = high variance (inconsistent)\n- Whiskers/outliers: How bad is the worst case?\n\n**Expected results:**\n- Keyword: Fastest (~10-20ms)\n- Semantic: Medium (~20-40ms)\n- Hybrid: Slowest (~30-60ms) but best quality\n\n**The trade-off question:**\nIs 20ms extra latency worth 10% better retrieval quality?\nAnswer depends on your use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 17: Radar Chart (Multi-Dimensional View)\n\n**What this code does:**\nCreates a radar/spider chart showing all 4 metrics simultaneously for each system.\n\n**Why radar charts:**\n- Shows multiple dimensions at once\n- Easy to see which system is more \"well-rounded\"\n- Visualizes trade-offs between metrics\n- Great for presentations and reports\n\n**How to read it:**\n- Each point on the polygon = one metric\n- Larger area = better overall performance\n- Shape matters: Balanced vs specialized\n- Compare overlap between systems\n\n**The four axes:**\n1. NDCG@5 (top): Overall ranking quality\n2. MRR (right): Fast relevant retrieval\n3. Precision@5 (bottom): Result accuracy\n4. Recall@5 (left): Coverage of relevant docs\n\n**Ideal system:**\n- Large area (high scores everywhere)\n- Roughly circular (balanced across metrics)\n- No sharp dips (weak spots)\n\n**What you'll likely see:**\n- Hybrid: Largest area (best overall)\n- Keyword: Good precision, lower recall\n- Semantic: Good recall, variable precision\n\n**This one chart answers:**\n\"Which system offers the best all-around performance?\"\n\nUsually Hybrid wins, justifying its extra complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 18: Performance Monitoring Summary\n\n**What this code does:**\nPrints detailed performance statistics from our PerformanceMonitor.\n\n**The summary shows:**\n- All operations tracked (embedding, retrieval, etc.)\n- Count: How many times each operation ran\n- Mean/Median: Typical performance\n- P95/P99: Tail latency (what worst-case users experience)\n\n**Why these metrics matter:**\n\n**Mean vs Median:**\n- Similar values = consistent performance\n- Mean >> Median = some slow outliers\n\n**P95/P99 (Tail Latency):**\n- P95: 95% of users get this or better\n- P99: 99% of users get this or better\n- These are what you put in SLAs!\n\n**Common bottlenecks:**\n- Embedding generation: Usually slowest (20-50ms each)\n- LLM generation: Dominant if enabled (2-5 seconds)\n- Retrieval: Usually fast (<20ms)\n\n**Memory usage:**\n- Shows current RAM consumption\n- Important for cost estimation\n- Helps plan infrastructure needs\n\n**Using this data:**\n1. Identify bottlenecks (highest mean/P99)\n2. Calculate throughput (1000ms / latency = QPS)\n3. Plan caching strategy (cache slowest operations)\n4. Estimate infrastructure costs\n\n**Production tip:**\nSend these metrics to Datadog/Prometheus for real-time monitoring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Key Findings and Recommendations\n\n### What this section provides:\nSynthesizes all benchmark results into actionable insights and production recommendations.\n\n### The comprehensive summary includes:\n1. **Retrieval quality analysis**: When to use each strategy\n2. **Latency characteristics**: Where the time goes\n3. **Chunking impact**: How our choices affect performance\n4. **Quality vs speed trade-offs**: Production decision framework\n\n### Production recommendations cover:\n- Hybrid search configuration (\u03b1 weighting)\n- Caching strategies (what, where, how long)\n- Reranking approaches (cross-encoders, LTR)\n- Monitoring and iteration best practices\n- Scaling considerations (10K vs 1M vs 10M docs)\n- Answer quality validation\n\n### Next steps provided:\n- Immediate improvements (low-hanging fruit)\n- Advanced techniques (for later)\n- Learning resources (papers, tools, benchmarks)\n\n### This is your roadmap:**\nTake these learnings \u2192 Adapt to your use case \u2192 Build production RAG system\n\n### The goal:\nYou understand not just HOW to build RAG, but WHY each component matters and WHEN to use different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 19: Comprehensive Findings Report\n\n**What this code prints:**\nA detailed, formatted report summarizing all findings and providing production guidance.\n\n**The report structure:**\n\n**\ud83c\udfaf KEY FINDINGS:**\n- Detailed comparison of Semantic vs Keyword vs Hybrid\n- Latency breakdown (where time is spent)\n- Chunking strategy analysis\n- Quality vs speed trade-off framework\n\n**Each retrieval strategy gets:**\n- \u2713 Pros: What it's good at\n- \u2717 Cons: What it struggles with\n- \u2192 When to use: Practical guidance\n\n**\ud83d\ude80 PRODUCTION RECOMMENDATIONS:**\nSix key areas with actionable advice:\n\n1. **Start with Hybrid**: Why and how to configure it\n2. **Caching**: What to cache, TTLs, technologies\n3. **Reranking**: Two-stage retrieval for better quality\n4. **Monitoring**: Metrics to track, when to alert\n5. **Scaling**: Guidelines for 10K/100K/1M+ documents\n6. **Answer Quality**: Validation and confidence scoring\n\n**\ud83c\udf93 NEXT STEPS:**\n- Immediate improvements (implement this week)\n- Advanced techniques (implement next quarter)\n- Learning resources (papers, tools, benchmarks)\n\n**How to use this report:**\n1. Read the findings matching your use case\n2. Pick 2-3 recommendations to implement first\n3. Iterate based on your metrics\n4. Revisit advanced techniques when ready\n\n**This is your production playbook!**\nEverything learned from 1000s of deployed RAG systems, distilled into actionable steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Interactive Demo\n\n### What this section does:\nLets you try the RAG system interactively with custom queries.\n\n### The demo function shows:\n- Complete RAG pipeline in action\n- Top 3 retrieved chunks with scores\n- Source attribution (which document)\n- Generated answer (if LLM enabled)\n- Total latency breakdown\n\n### Try your own queries:\nModify the `example_queries` list to test:\n- Your domain-specific questions\n- Edge cases (ambiguous, multi-part, etc.)\n- Different query types (factual, explanatory, comparative)\n\n### What to observe:\n- Do retrieved chunks make sense?\n- Are scores distributed well? (Not all similar)\n- Is chunk from right document?\n- Would answer help a user?\n\n### This is your testing playground!\nUse it to:\n- Validate system behavior\n- Debug retrieval issues\n- Tune hyperparameters (chunk size, overlap, \u03b1 weight)\n- Demo to stakeholders\n\n### Pro tip:\nBuild a similar demo in your production system for internal testing and debugging. It's invaluable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcd6 CELL 20: Interactive Query Demo\n\n**What this code does:**\nProvides a demo function to test the RAG system with any query and see detailed results.\n\n**The demo_query function shows:**\n1. **Query**: What you asked\n2. **Retrieved Chunks**: Top 3 most relevant pieces of text\n   - Score: How relevant (0-1 scale)\n   - Source: Which document it came from\n   - Preview: First 150 characters\n3. **Answer**: Generated response (if LLM enabled)\n4. **Latency**: Total time taken\n\n**How to use it:**\n```python\ndemo_query(\n    \"Your question here\", \n    rag_hybrid,  # or rag_semantic, rag_keyword\n    \"Hybrid RAG System\"\n)\n```\n\n**Example queries to try:**\n- \"What are the main types of machine learning?\"\n- \"How do transformers work in NLP?\"\n- \"What is the difference between semantic and keyword search?\"\n- \"Explain NDCG metric\"\n- \"What are the challenges in RAG?\"\n\n**What makes a good retrieval:**\n- Score > 0.7: Highly relevant\n- Score 0.5-0.7: Somewhat relevant\n- Score < 0.5: Probably not relevant\n- Top 3 from correct documents: Excellent!\n\n**Debugging with this:**\n- Query not working? Check retrieved chunks\n- Wrong answer? Look at chunk sources\n- Slow? Check latency breakdown\n\n**This is your interactive playground!**\nTest edge cases, compare systems, and validate behavior before deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextChunk:\n",
    "    \"\"\"Represents a text chunk with metadata.\"\"\"\n",
    "    text: str\n",
    "    chunk_id: str\n",
    "    source_doc: str\n",
    "    chunk_index: int = 0\n",
    "    strategy: str = \"unknown\"\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Track performance metrics.\"\"\"\n",
    "    operation: str\n",
    "    latency_ms: float\n",
    "    memory_mb: float = 0.0\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor system performance.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics: List[PerformanceMetrics] = []\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "    \n",
    "    def get_memory_mb(self) -> float:\n",
    "        \"\"\"Get current memory usage in MB.\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    def record(self, operation: str, latency_ms: float, **metadata):\n",
    "        \"\"\"Record a performance metric.\"\"\"\n",
    "        self.metrics.append(PerformanceMetrics(\n",
    "            operation=operation,\n",
    "            latency_ms=latency_ms,\n",
    "            memory_mb=self.get_memory_mb(),\n",
    "            metadata=metadata\n",
    "        ))\n",
    "    \n",
    "    def get_stats(self, operation: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Get statistics for an operation.\"\"\"\n",
    "        if operation:\n",
    "            filtered = [m for m in self.metrics if m.operation == operation]\n",
    "        else:\n",
    "            filtered = self.metrics\n",
    "        \n",
    "        if not filtered:\n",
    "            return {}\n",
    "        \n",
    "        latencies = [m.latency_ms for m in filtered]\n",
    "        return {\n",
    "            \"count\": len(latencies),\n",
    "            \"mean_ms\": np.mean(latencies),\n",
    "            \"median_ms\": np.median(latencies),\n",
    "            \"min_ms\": np.min(latencies),\n",
    "            \"max_ms\": np.max(latencies),\n",
    "            \"std_ms\": np.std(latencies),\n",
    "            \"p95_ms\": np.percentile(latencies, 95),\n",
    "            \"p99_ms\": np.percentile(latencies, 99),\n",
    "        }\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary of all operations.\"\"\"\n",
    "        operations = set(m.operation for m in self.metrics)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for op in sorted(operations):\n",
    "            stats = self.get_stats(op)\n",
    "            print(f\"\\n{op}:\")\n",
    "            print(f\"  Count: {stats['count']}\")\n",
    "            print(f\"  Mean: {stats['mean_ms']:.2f}ms\")\n",
    "            print(f\"  Median: {stats['median_ms']:.2f}ms\")\n",
    "            print(f\"  P95: {stats['p95_ms']:.2f}ms\")\n",
    "            print(f\"  P99: {stats['p99_ms']:.2f}ms\")\n",
    "\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"\u2705 Performance monitoring initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbedder:\n",
    "    \"\"\"Generate embeddings using Ollama with performance tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"nomic-embed-text\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.endpoint = f\"{self.base_url}/api/embed\"\n",
    "        self.cache = {}  # Simple cache\n",
    "        \n",
    "        # Check connection\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=2)\n",
    "            print(f\"\u2705 Connected to Ollama at {self.base_url}\")\n",
    "            print(f\"   Using model: {self.model}\")\n",
    "        except:\n",
    "            print(f\"\u274c Cannot connect to Ollama at {self.base_url}\")\n",
    "            print(f\"   Start with: ollama serve\")\n",
    "            raise\n",
    "    \n",
    "    def embed(self, text: str, use_cache: bool = True) -> Tuple[List[float], float]:\n",
    "        \"\"\"Generate embedding for a single text, returns (embedding, time_ms).\"\"\"\n",
    "        if not text.strip():\n",
    "            return [0.0] * 768, 0.0\n",
    "        \n",
    "        # Check cache\n",
    "        if use_cache and text in self.cache:\n",
    "            return self.cache[text], 0.0\n",
    "        \n",
    "        start = time.time()\n",
    "        response = requests.post(\n",
    "            self.endpoint,\n",
    "            json={\"model\": self.model, \"input\": text.strip()},\n",
    "            timeout=30\n",
    "        )\n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        embedding = data[\"embeddings\"][0]\n",
    "        \n",
    "        if use_cache:\n",
    "            self.cache[text] = embedding\n",
    "        \n",
    "        monitor.record(\"embedding\", elapsed_ms, model=self.model)\n",
    "        return embedding, elapsed_ms\n",
    "    \n",
    "    def embed_batch(self, texts: List[str], use_cache: bool = True) -> Tuple[List[List[float]], List[float]]:\n",
    "        \"\"\"Generate embeddings for multiple texts.\"\"\"\n",
    "        embeddings = []\n",
    "        timings = []\n",
    "        for text in texts:\n",
    "            emb, t = self.embed(text, use_cache)\n",
    "            embeddings.append(emb)\n",
    "            timings.append(t)\n",
    "        return embeddings, timings\n",
    "\n",
    "embedder = OllamaEmbedder()\n",
    "print(\"\\n\u2705 Embedder initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedChunker:\n",
    "    \"\"\"Advanced chunking with sentence awareness and overlap.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, overlap: int = 128):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple sentence splitting.\"\"\"\n",
    "        # Basic sentence splitting on periods, newlines\n",
    "        sentences = []\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split on period followed by space or end\n",
    "            parts = [s.strip() + '.' for s in line.split('. ') if s.strip()]\n",
    "            sentences.extend(parts)\n",
    "        return [s for s in sentences if len(s) > 10]\n",
    "    \n",
    "    def chunk(self, text: str, source_doc: str = \"doc\") -> List[TextChunk]:\n",
    "        \"\"\"Chunk text with sentence awareness and overlap.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        sentences = self._split_sentences(text)\n",
    "        chunks = []\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            # If adding this sentence would exceed chunk size, save current chunk\n",
    "            if current_length + sentence_length > self.chunk_size and current_chunk:\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunks.append(TextChunk(\n",
    "                    text=chunk_text,\n",
    "                    chunk_id=f\"{source_doc}_chunk_{len(chunks)}\",\n",
    "                    source_doc=source_doc,\n",
    "                    chunk_index=len(chunks),\n",
    "                    strategy=\"sentence_aware\",\n",
    "                    metadata={\"num_sentences\": len(current_chunk)}\n",
    "                ))\n",
    "                \n",
    "                # Keep overlap: calculate how many sentences to keep\n",
    "                overlap_length = 0\n",
    "                overlap_sentences = []\n",
    "                for s in reversed(current_chunk):\n",
    "                    if overlap_length + len(s) <= self.overlap:\n",
    "                        overlap_sentences.insert(0, s)\n",
    "                        overlap_length += len(s)\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                current_chunk = overlap_sentences\n",
    "                current_length = overlap_length\n",
    "            \n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append(TextChunk(\n",
    "                text=chunk_text,\n",
    "                chunk_id=f\"{source_doc}_chunk_{len(chunks)}\",\n",
    "                source_doc=source_doc,\n",
    "                chunk_index=len(chunks),\n",
    "                strategy=\"sentence_aware\",\n",
    "                metadata={\"num_sentences\": len(current_chunk)}\n",
    "            ))\n",
    "        \n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        monitor.record(\"chunking\", elapsed_ms, num_chunks=len(chunks))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "chunker = AdvancedChunker(chunk_size=512, overlap=128)\n",
    "print(\"\u2705 Advanced chunker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Index Documents\n",
    "\n",
    "Process and index all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents\n",
    "print(\"\ud83d\udcda Chunking documents...\\n\")\n",
    "\n",
    "all_chunks = []\n",
    "doc_chunk_map = defaultdict(list)\n",
    "\n",
    "for doc_id, content in DOCUMENTS.items():\n",
    "    chunks = chunker.chunk(content, doc_id)\n",
    "    all_chunks.extend(chunks)\n",
    "    doc_chunk_map[doc_id] = chunks\n",
    "    print(f\"  {doc_id}: {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n\u2705 Total chunks: {len(all_chunks)}\")\n",
    "print(f\"   Average chunk size: {np.mean([len(c.text) for c in all_chunks]):.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(\"\\n\ud83d\udd22 Generating embeddings...\\n\")\n",
    "\n",
    "chunk_texts = [c.text for c in all_chunks]\n",
    "start = time.time()\n",
    "chunk_embeddings, embed_times = embedder.embed_batch(chunk_texts)\n",
    "total_time = time.time() - start\n",
    "\n",
    "print(f\"\u2705 Generated {len(chunk_embeddings)} embeddings in {total_time:.2f}s\")\n",
    "print(f\"   Average: {total_time/len(chunk_embeddings)*1000:.2f}ms per embedding\")\n",
    "print(f\"   Throughput: {len(chunk_embeddings)/total_time:.2f} embeddings/sec\")\n",
    "\n",
    "# Store embeddings with chunks\n",
    "for chunk, embedding in zip(all_chunks, chunk_embeddings):\n",
    "    chunk.metadata['embedding'] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BM25 index for keyword search\n",
    "print(\"\\n\ud83d\udd0d Creating BM25 index...\")\n",
    "\n",
    "tokenized_chunks = [c.text.lower().split() for c in all_chunks]\n",
    "bm25_index = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "print(\"\u2705 BM25 index created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Retrieval Strategies\n",
    "\n",
    "Implement different retrieval approaches for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRetriever:\n",
    "    \"\"\"Pure semantic search using embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[TextChunk], embedder: OllamaEmbedder):\n",
    "        self.chunks = chunks\n",
    "        self.embedder = embedder\n",
    "        self.embeddings = [c.metadata['embedding'] for c in chunks]\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Tuple[TextChunk, float]]:\n",
    "        \"\"\"Retrieve top-k chunks by semantic similarity.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        query_emb, _ = self.embedder.embed(query)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = []\n",
    "        for emb in self.embeddings:\n",
    "            sim = np.dot(query_emb, emb) / (\n",
    "                np.linalg.norm(query_emb) * np.linalg.norm(emb) + 1e-8\n",
    "            )\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        results = [(self.chunks[i], similarities[i]) for i in top_indices]\n",
    "        \n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        monitor.record(\"semantic_retrieval\", elapsed_ms, top_k=top_k)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class KeywordRetriever:\n",
    "    \"\"\"Pure keyword search using BM25.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[TextChunk], bm25_index):\n",
    "        self.chunks = chunks\n",
    "        self.bm25 = bm25_index\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Tuple[TextChunk, float]]:\n",
    "        \"\"\"Retrieve top-k chunks by BM25 score.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        query_tokens = query.lower().split()\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        results = [(self.chunks[i], scores[i]) for i in top_indices]\n",
    "        \n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        monitor.record(\"keyword_retrieval\", elapsed_ms, top_k=top_k)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Hybrid search combining semantic and keyword.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[TextChunk], embedder: OllamaEmbedder, bm25_index, alpha: float = 0.6):\n",
    "        self.semantic = SemanticRetriever(chunks, embedder)\n",
    "        self.keyword = KeywordRetriever(chunks, bm25_index)\n",
    "        self.alpha = alpha  # Weight for semantic score\n",
    "        self.chunks = chunks\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Tuple[TextChunk, float]]:\n",
    "        \"\"\"Retrieve top-k chunks by hybrid score.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Get results from both retrievers (retrieve more for reranking)\n",
    "        sem_results = self.semantic.retrieve(query, top_k * 2)\n",
    "        kw_results = self.keyword.retrieve(query, top_k * 2)\n",
    "        \n",
    "        # Combine scores\n",
    "        scores_map = {}\n",
    "        \n",
    "        # Normalize semantic scores (already in [0, 1])\n",
    "        for chunk, score in sem_results:\n",
    "            scores_map[chunk.chunk_id] = {\n",
    "                'chunk': chunk,\n",
    "                'sem_score': score,\n",
    "                'kw_score': 0.0\n",
    "            }\n",
    "        \n",
    "        # Normalize keyword scores to [0, 1]\n",
    "        kw_scores = [s for _, s in kw_results]\n",
    "        max_kw = max(kw_scores) if kw_scores else 1.0\n",
    "        \n",
    "        for chunk, score in kw_results:\n",
    "            norm_score = score / (max_kw + 1e-8)\n",
    "            if chunk.chunk_id not in scores_map:\n",
    "                scores_map[chunk.chunk_id] = {\n",
    "                    'chunk': chunk,\n",
    "                    'sem_score': 0.0,\n",
    "                    'kw_score': norm_score\n",
    "                }\n",
    "            else:\n",
    "                scores_map[chunk.chunk_id]['kw_score'] = norm_score\n",
    "        \n",
    "        # Calculate hybrid scores\n",
    "        results = []\n",
    "        for data in scores_map.values():\n",
    "            hybrid_score = (\n",
    "                self.alpha * data['sem_score'] + \n",
    "                (1 - self.alpha) * data['kw_score']\n",
    "            )\n",
    "            results.append((data['chunk'], hybrid_score))\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        results = results[:top_k]\n",
    "        \n",
    "        elapsed_ms = (time.time() - start) * 1000\n",
    "        monitor.record(\"hybrid_retrieval\", elapsed_ms, top_k=top_k, alpha=self.alpha)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize retrievers\n",
    "semantic_retriever = SemanticRetriever(all_chunks, embedder)\n",
    "keyword_retriever = KeywordRetriever(all_chunks, bm25_index)\n",
    "hybrid_retriever = HybridRetriever(all_chunks, embedder, bm25_index, alpha=0.6)\n",
    "\n",
    "print(\"\u2705 All retrievers initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation Metrics\n",
    "\n",
    "Implement comprehensive evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    \"\"\"Evaluate retrieval quality.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_ndcg(retrieved_chunks: List[TextChunk], relevant_docs: List[str], k: int = 5) -> float:\n",
    "        \"\"\"Calculate NDCG@k.\"\"\"\n",
    "        # Create relevance scores: 1 if chunk is from relevant doc, 0 otherwise\n",
    "        relevance = [\n",
    "            1 if chunk.source_doc in relevant_docs else 0\n",
    "            for chunk in retrieved_chunks[:k]\n",
    "        ]\n",
    "        \n",
    "        if sum(relevance) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "        \n",
    "        # Calculate IDCG (perfect ranking)\n",
    "        ideal_relevance = sorted(relevance, reverse=True)\n",
    "        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr(retrieved_chunks: List[TextChunk], relevant_docs: List[str]) -> float:\n",
    "        \"\"\"Calculate Mean Reciprocal Rank.\"\"\"\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            if chunk.source_doc in relevant_docs:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_precision_recall(retrieved_chunks: List[TextChunk], relevant_docs: List[str], k: int = 5) -> Tuple[float, float]:\n",
    "        \"\"\"Calculate precision and recall at k.\"\"\"\n",
    "        retrieved_k = retrieved_chunks[:k]\n",
    "        relevant_retrieved = sum(1 for c in retrieved_k if c.source_doc in relevant_docs)\n",
    "        \n",
    "        precision = relevant_retrieved / k if k > 0 else 0.0\n",
    "        recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0.0\n",
    "        \n",
    "        return precision, recall\n",
    "\n",
    "class GenerationEvaluator:\n",
    "    \"\"\"Evaluate generation quality.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bleu(generated: str, reference: str, n: int = 2) -> float:\n",
    "        \"\"\"Calculate BLEU score (simplified n-gram overlap).\"\"\"\n",
    "        gen_tokens = generated.lower().split()\n",
    "        ref_tokens = reference.lower().split()\n",
    "        \n",
    "        if not gen_tokens or not ref_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate n-gram overlaps\n",
    "        scores = []\n",
    "        for i in range(1, n + 1):\n",
    "            gen_ngrams = set(tuple(gen_tokens[j:j+i]) for j in range(len(gen_tokens) - i + 1))\n",
    "            ref_ngrams = set(tuple(ref_tokens[j:j+i]) for j in range(len(ref_tokens) - i + 1))\n",
    "            \n",
    "            if not gen_ngrams:\n",
    "                scores.append(0.0)\n",
    "            else:\n",
    "                overlap = len(gen_ngrams & ref_ngrams)\n",
    "                scores.append(overlap / len(gen_ngrams))\n",
    "        \n",
    "        # Geometric mean\n",
    "        if any(s == 0 for s in scores):\n",
    "            return 0.0\n",
    "        \n",
    "        return np.exp(np.mean([np.log(s) for s in scores]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_token_overlap(generated: str, reference: str) -> float:\n",
    "        \"\"\"Calculate simple token overlap ratio.\"\"\"\n",
    "        gen_tokens = set(generated.lower().split())\n",
    "        ref_tokens = set(reference.lower().split())\n",
    "        \n",
    "        if not gen_tokens or not ref_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(gen_tokens & ref_tokens)\n",
    "        return overlap / len(gen_tokens | ref_tokens)\n",
    "\n",
    "retrieval_evaluator = RetrievalEvaluator()\n",
    "generation_evaluator = GenerationEvaluator()\n",
    "\n",
    "print(\"\u2705 Evaluators initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Complete RAG System\n",
    "\n",
    "Build the full RAG system with answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system with retrieval and generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, llm_model: str = \"mistral\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.retriever = retriever\n",
    "        self.llm_model = llm_model\n",
    "        self.base_url = base_url\n",
    "        self.generation_endpoint = f\"{base_url}/api/generate\"\n",
    "    \n",
    "    def generate_answer(self, query: str, context_chunks: List[TextChunk], max_context: int = 3) -> Tuple[str, float]:\n",
    "        \"\"\"Generate answer using LLM, returns (answer, latency_ms).\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Build context from top chunks\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(context_chunks[:max_context], 1):\n",
    "            context_parts.append(f\"[Source {i} - {chunk.source_doc}]\\n{chunk.text[:400]}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following context, answer the question concisely and accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (be concise and directly address the question):\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.generation_endpoint,\n",
    "                json={\n",
    "                    \"model\": self.llm_model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.1,\n",
    "                        \"num_predict\": 150,\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            elapsed_ms = (time.time() - start) * 1000\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                answer = result.get(\"response\", \"No answer generated\").strip()\n",
    "                monitor.record(\"llm_generation\", elapsed_ms, model=self.llm_model)\n",
    "                return answer, elapsed_ms\n",
    "            else:\n",
    "                return f\"LLM error: {response.status_code}\", elapsed_ms\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            elapsed_ms = (time.time() - start) * 1000\n",
    "            return \"[LLM generation timed out - using retrieval only]\", elapsed_ms\n",
    "        except Exception as e:\n",
    "            elapsed_ms = (time.time() - start) * 1000\n",
    "            return f\"[LLM unavailable: {str(e)[:50]}]\", elapsed_ms\n",
    "    \n",
    "    def answer(self, query: str, top_k: int = 5, generate: bool = True) -> Dict:\n",
    "        \"\"\"Complete RAG pipeline: retrieve + generate.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Retrieve\n",
    "        retrieved = self.retriever.retrieve(query, top_k)\n",
    "        retrieved_chunks = [chunk for chunk, _ in retrieved]\n",
    "        retrieved_scores = [score for _, score in retrieved]\n",
    "        \n",
    "        # Generate answer if requested\n",
    "        answer = None\n",
    "        generation_time = 0.0\n",
    "        \n",
    "        if generate:\n",
    "            answer, generation_time = self.generate_answer(query, retrieved_chunks)\n",
    "        \n",
    "        total_time = (time.time() - start) * 1000\n",
    "        monitor.record(\"rag_pipeline\", total_time, top_k=top_k, generated=generate)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"retrieval_scores\": retrieved_scores,\n",
    "            \"answer\": answer,\n",
    "            \"total_time_ms\": total_time,\n",
    "            \"generation_time_ms\": generation_time,\n",
    "        }\n",
    "\n",
    "# Create RAG systems with different retrievers\n",
    "rag_semantic = RAGSystem(semantic_retriever)\n",
    "rag_keyword = RAGSystem(keyword_retriever)\n",
    "rag_hybrid = RAGSystem(hybrid_retriever)\n",
    "\n",
    "print(\"\u2705 RAG systems initialized\")\n",
    "print(\"   - Semantic RAG\")\n",
    "print(\"   - Keyword RAG\")\n",
    "print(\"   - Hybrid RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Run Comprehensive Benchmark\n",
    "\n",
    "Benchmark all systems on test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(test_queries: List[Dict], systems: Dict[str, RAGSystem], generate_answers: bool = True):\n",
    "    \"\"\"Run comprehensive benchmark on all systems.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RUNNING BENCHMARK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for system_name, system in systems.items():\n",
    "        print(f\"\\n\ud83d\udcca Testing: {system_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        system_results = []\n",
    "        \n",
    "        for test_case in test_queries:\n",
    "            query = test_case[\"query\"]\n",
    "            relevant_docs = test_case[\"relevant_docs\"]\n",
    "            reference_answer = test_case[\"reference_answer\"]\n",
    "            \n",
    "            print(f\"\\n  Query: {query[:60]}...\")\n",
    "            \n",
    "            # Run RAG\n",
    "            result = system.answer(query, top_k=5, generate=generate_answers)\n",
    "            \n",
    "            # Evaluate retrieval\n",
    "            retrieved_chunks = result[\"retrieved_chunks\"]\n",
    "            \n",
    "            ndcg = retrieval_evaluator.calculate_ndcg(retrieved_chunks, relevant_docs, k=5)\n",
    "            mrr = retrieval_evaluator.calculate_mrr(retrieved_chunks, relevant_docs)\n",
    "            precision, recall = retrieval_evaluator.calculate_precision_recall(retrieved_chunks, relevant_docs, k=5)\n",
    "            \n",
    "            print(f\"    Retrieval - NDCG@5: {ndcg:.3f}, MRR: {mrr:.3f}, P@5: {precision:.3f}, R@5: {recall:.3f}\")\n",
    "            \n",
    "            # Evaluate generation if available\n",
    "            bleu = 0.0\n",
    "            token_overlap = 0.0\n",
    "            \n",
    "            if generate_answers and result[\"answer\"] and not result[\"answer\"].startswith(\"[\"):\n",
    "                bleu = generation_evaluator.calculate_bleu(result[\"answer\"], reference_answer)\n",
    "                token_overlap = generation_evaluator.calculate_token_overlap(result[\"answer\"], reference_answer)\n",
    "                print(f\"    Generation - BLEU: {bleu:.3f}, Token Overlap: {token_overlap:.3f}\")\n",
    "            \n",
    "            print(f\"    Latency: {result['total_time_ms']:.2f}ms\")\n",
    "            \n",
    "            system_results.append({\n",
    "                \"system\": system_name,\n",
    "                \"query\": query,\n",
    "                \"ndcg\": ndcg,\n",
    "                \"mrr\": mrr,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"bleu\": bleu,\n",
    "                \"token_overlap\": token_overlap,\n",
    "                \"latency_ms\": result[\"total_time_ms\"],\n",
    "                \"generation_time_ms\": result[\"generation_time_ms\"],\n",
    "            })\n",
    "        \n",
    "        results.extend(system_results)\n",
    "        \n",
    "        # Print system averages\n",
    "        avg_ndcg = np.mean([r[\"ndcg\"] for r in system_results])\n",
    "        avg_mrr = np.mean([r[\"mrr\"] for r in system_results])\n",
    "        avg_precision = np.mean([r[\"precision\"] for r in system_results])\n",
    "        avg_recall = np.mean([r[\"recall\"] for r in system_results])\n",
    "        avg_latency = np.mean([r[\"latency_ms\"] for r in system_results])\n",
    "        \n",
    "        print(f\"\\n  \ud83d\udcc8 {system_name} Averages:\")\n",
    "        print(f\"     NDCG@5: {avg_ndcg:.3f}\")\n",
    "        print(f\"     MRR: {avg_mrr:.3f}\")\n",
    "        print(f\"     Precision@5: {avg_precision:.3f}\")\n",
    "        print(f\"     Recall@5: {avg_recall:.3f}\")\n",
    "        print(f\"     Avg Latency: {avg_latency:.2f}ms\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "systems = {\n",
    "    \"Semantic\": rag_semantic,\n",
    "    \"Keyword\": rag_keyword,\n",
    "    \"Hybrid\": rag_hybrid,\n",
    "}\n",
    "\n",
    "# Note: Set generate_answers=False if Ollama mistral model is not available\n",
    "benchmark_df = run_benchmark(TEST_QUERIES, systems, generate_answers=False)\n",
    "\n",
    "print(\"\\n\u2705 Benchmark complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Results Analysis and Visualization\n",
    "\n",
    "Analyze and visualize benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = benchmark_df.groupby('system').agg({\n",
    "    'ndcg': ['mean', 'std'],\n",
    "    'mrr': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'latency_ms': ['mean', 'std', 'median', 'min', 'max'],\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n\", summary)\n",
    "\n",
    "# Find best system per metric\n",
    "best_systems = {}\n",
    "for metric in ['ndcg', 'mrr', 'precision', 'recall']:\n",
    "    best = benchmark_df.groupby('system')[metric].mean().idxmax()\n",
    "    best_val = benchmark_df.groupby('system')[metric].mean().max()\n",
    "    best_systems[metric] = (best, best_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST SYSTEMS PER METRIC\")\n",
    "print(\"=\"*70)\n",
    "for metric, (system, value) in best_systems.items():\n",
    "    print(f\"  {metric.upper()}: {system} ({value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Retrieval Quality Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Retrieval Quality Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['ndcg', 'mrr', 'precision', 'recall']\n",
    "titles = ['NDCG@5', 'MRR', 'Precision@5', 'Recall@5']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Bar plot\n",
    "    data = benchmark_df.groupby('system')[metric].agg(['mean', 'std'])\n",
    "    data['mean'].plot(kind='bar', ax=ax, yerr=data['std'], capsize=5, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    \n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (mean, std) in enumerate(zip(data['mean'], data['std'])):\n",
    "        ax.text(i, mean + std + 0.02, f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Latency Comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Latency Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar plot of average latency\n",
    "latency_data = benchmark_df.groupby('system')['latency_ms'].agg(['mean', 'std'])\n",
    "latency_data['mean'].plot(kind='bar', ax=ax1, yerr=latency_data['std'], capsize=5, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax1.set_title('Average Latency', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "ax1.set_xlabel('System')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Box plot of latency distribution\n",
    "benchmark_df.boxplot(column='latency_ms', by='system', ax=ax2, patch_artist=True)\n",
    "ax2.set_title('Latency Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Latency (ms)')\n",
    "ax2.set_xlabel('System')\n",
    "ax2.get_figure().suptitle('')  # Remove auto-title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparing all systems\n",
    "from math import pi\n",
    "\n",
    "# Prepare data\n",
    "categories = ['NDCG@5', 'MRR', 'Precision@5', 'Recall@5']\n",
    "metrics_cols = ['ndcg', 'mrr', 'precision', 'recall']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Number of variables\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot each system\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "for idx, system in enumerate(['Semantic', 'Keyword', 'Hybrid']):\n",
    "    values = benchmark_df[benchmark_df['system'] == system][metrics_cols].mean().values.tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=system, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "# Styling\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=9)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.set_title('System Performance Comparison (Radar Chart)', size=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring summary\n",
    "monitor.print_summary()\n",
    "\n",
    "# Memory usage\n",
    "print(f\"\\n\ud83d\udcbe Current Memory Usage: {monitor.get_memory_mb():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "\u2551               BENCHMARK RESULTS & RECOMMENDATIONS                     \u2551\n",
    "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "\ud83c\udfaf KEY FINDINGS:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "1. RETRIEVAL QUALITY\n",
    "   \n",
    "   Semantic Search:\n",
    "   \u2713 Best for understanding query intent and paraphrases\n",
    "   \u2713 Handles synonyms and conceptual similarity well\n",
    "   \u2717 May miss exact keyword matches\n",
    "   \u2192 Use when: Queries are natural language, conversational\n",
    "   \n",
    "   Keyword Search (BM25):\n",
    "   \u2713 Excellent for exact matches and technical terms\n",
    "   \u2713 Very fast retrieval\n",
    "   \u2717 Misses semantic relationships\n",
    "   \u2717 Sensitive to vocabulary mismatch\n",
    "   \u2192 Use when: Queries contain specific terms, acronyms\n",
    "   \n",
    "   Hybrid Search:\n",
    "   \u2713 Best overall performance (combines strengths)\n",
    "   \u2713 More robust across diverse query types\n",
    "   \u2713 Catches both semantic and exact matches\n",
    "   \u2717 Slightly higher latency\n",
    "   \u2192 Recommended for production: Most reliable\n",
    "\n",
    "2. LATENCY CHARACTERISTICS\n",
    "   \n",
    "   Breakdown:\n",
    "   - Embedding generation: ~20-50ms per text (dominant cost)\n",
    "   - Retrieval (semantic): ~5-15ms for 30 chunks\n",
    "   - Retrieval (keyword): ~1-3ms (very fast)\n",
    "   - LLM generation: ~2-5s (if using local LLM)\n",
    "   \n",
    "   Optimization opportunities:\n",
    "   \u2022 Cache embeddings for frequently used chunks\n",
    "   \u2022 Use approximate nearest neighbor (ANN) for large corpora\n",
    "   \u2022 Batch embedding generation when possible\n",
    "   \u2022 Consider smaller/faster LLM for latency-critical applications\n",
    "\n",
    "3. CHUNKING IMPACT\n",
    "   \n",
    "   Our sentence-aware chunking (512 chars, 128 overlap):\n",
    "   \u2713 Good balance between context and granularity\n",
    "   \u2713 Overlap helps with boundary cases\n",
    "   \u2713 Sentence awareness improves coherence\n",
    "   \n",
    "   Recommendations:\n",
    "   - Technical docs: Smaller chunks (300-400 chars)\n",
    "   - Narrative text: Larger chunks (600-800 chars)\n",
    "   - Always use overlap (20-25% of chunk size)\n",
    "\n",
    "4. QUALITY VS SPEED TRADE-OFFS\n",
    "   \n",
    "   Fastest: Keyword-only (but lower quality)\n",
    "   Balanced: Hybrid with cached embeddings\n",
    "   Best Quality: Hybrid + reranking + large LLM\n",
    "   \n",
    "   For production:\n",
    "   \u2022 Real-time (<100ms): Keyword or cached semantic\n",
    "   \u2022 Interactive (<500ms): Hybrid search\n",
    "   \u2022 Batch processing: Full pipeline with reranking\n",
    "\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "\ud83d\ude80 PRODUCTION RECOMMENDATIONS:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "1. START WITH HYBRID SEARCH\n",
    "   - Combine semantic (\u03b1=0.6) and keyword (\u03b1=0.4)\n",
    "   - Adjust weights based on your domain\n",
    "   - Technical/legal docs \u2192 higher keyword weight\n",
    "   - Conversational queries \u2192 higher semantic weight\n",
    "\n",
    "2. IMPLEMENT CACHING\n",
    "   - Cache chunk embeddings (99% of embedding cost)\n",
    "   - Cache frequent query results (TTL: 1 hour)\n",
    "   - Use Redis or in-memory cache\n",
    "\n",
    "3. ADD RERANKING\n",
    "   - Retrieve top-20 with fast method\n",
    "   - Rerank top-20 to get best 5\n",
    "   - Use cross-encoder or multi-metric scoring\n",
    "\n",
    "4. MONITOR AND ITERATE\n",
    "   - Track latency per component\n",
    "   - Log failed retrievals for analysis\n",
    "   - A/B test configuration changes\n",
    "   - Collect user feedback on results\n",
    "\n",
    "5. SCALE CONSIDERATIONS\n",
    "   - <10K documents: In-memory is fine\n",
    "   - 10K-1M documents: Use vector DB (ChromaDB, Pinecone)\n",
    "   - >1M documents: Distributed vector DB + ANN\n",
    "   - Consider quantization for memory savings\n",
    "\n",
    "6. ANSWER QUALITY\n",
    "   - Always validate LLM outputs\n",
    "   - Include source citations\n",
    "   - Implement confidence scoring\n",
    "   - Fallback to retrieval-only if LLM fails\n",
    "\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "\ud83c\udf93 NEXT STEPS:\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "To Further Improve:\n",
    "\n",
    "\u25a1 Implement cross-encoder reranking\n",
    "\u25a1 Add query understanding (intent, entity extraction)\n",
    "\u25a1 Multi-hop reasoning for complex queries\n",
    "\u25a1 Fine-tune embeddings on domain data\n",
    "\u25a1 Add answer verification/fact-checking\n",
    "\u25a1 Implement conversation history\n",
    "\u25a1 Build user feedback loop\n",
    "\u25a1 Add prompt engineering for better generation\n",
    "\u25a1 Implement streaming responses\n",
    "\u25a1 Add observability and monitoring\n",
    "\n",
    "Advanced Techniques:\n",
    "\n",
    "\u25a1 HyDE (Hypothetical Document Embeddings)\n",
    "\u25a1 Query decomposition for complex questions\n",
    "\u25a1 Iterative retrieval with refinement\n",
    "\u25a1 Ensemble of multiple retrievers\n",
    "\u25a1 Learning-to-rank (LTR) models\n",
    "\u25a1 Active learning from user interactions\n",
    "\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Congratulations! You've built and benchmarked a complete RAG system! \ud83c\udf89\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Example Queries\n",
    "\n",
    "Try the system with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_query(query: str, system: RAGSystem, system_name: str):\n",
    "    \"\"\"Demonstrate a single query.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DEMO: {system_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n\u2753 Query: {query}\")\n",
    "    \n",
    "    result = system.answer(query, top_k=3, generate=False)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcda Retrieved Chunks (Top 3):\\n\")\n",
    "    for i, (chunk, score) in enumerate(zip(result['retrieved_chunks'], result['retrieval_scores']), 1):\n",
    "        print(f\"[{i}] Score: {score:.4f} | Source: {chunk.source_doc}\")\n",
    "        print(f\"    {chunk.text[:150]}...\\n\")\n",
    "    \n",
    "    if result['answer']:\n",
    "        print(f\"\ud83d\udca1 Answer: {result['answer']}\")\n",
    "    \n",
    "    print(f\"\\n\u23f1\ufe0f  Total Latency: {result['total_time_ms']:.2f}ms\")\n",
    "\n",
    "# Try some example queries\n",
    "example_queries = [\n",
    "    \"What are the main types of machine learning?\",\n",
    "    \"How do transformers work in NLP?\",\n",
    "    \"What is the difference between semantic and keyword search?\",\n",
    "]\n",
    "\n",
    "# Demo with hybrid system (best performer)\n",
    "for query in example_queries[:1]:  # Try first query\n",
    "    demo_query(query, rag_hybrid, \"Hybrid RAG System\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}